# Introduction

## Linear regression overview

Regression is the predicting or learning of numerical features in statistics and machine learning. This process includes the prediction of numeric as well as categorical attributes. Performing a regression allows for the determination of which factors in situations matter, what is their impact, and how they influence other factors. Regression allows for data driven decisions that eliminate guesswork. [@Joshi20]

Simple linear regression finds the relationship between a dependent and independent variable whereas multiple linear regression can find the relationship between the dependent and multiple independent variables. [@Joshi20]

The data is modeled on a graph and a line is fitted to the data. This allows for the determination of the coefficients of the independent variable or variables. [@Joshi20]

### Regression line fitting

Regression line fitting is a delicate balance as the line should predict the desired value well based on several features but be applicable enough to be used repeatedly with different samples of data. The line should not be underfitted, which would not provide an accurate prediction. The line also should not be overfitted to the sample, which would cause the model not to work well with different samples and the data at large. [@Jabbar14]

Several methods can be used to ensure appropriate line fitting. We can compare the data to the AUC Curves which allows for determining the curves predictive ability by quantifying it. The higher the AUC the more predictive the model. The data is separated into two parts. 80% of the data is labeled as the training or estimation data and 20% of the data is labeled as the validation set. Both parts of the data are tested to ensure the AUC values are similar.[@Aheto21]

The Penalty Method is important to prevent overfitting. We set $E_{train}$ and $E_{test}$ to be the test error, respectively. We then try to find a the minimal penalty such that $E_{test} = E_{train} + Penalty$ [@Jabbar14]

Early Stopping Method is used for the prevention of both over and underfitting. The sample data is broken down into three parts: training, validation, and testing. This data is then broken down in to the similar 80%/20% split to estimate the line and then to validate the process. [@Jabbar14]

### Bias and variance

Bias measures the amount of deviation from the expected value is from the estimator in the models. Variance measures how far the data fluctuates. (Mehta, Bukov, Wang)

If we consider the bulls eye as an example: low bias indicates that the estimators are in the vicinity of the bulls eye and low variance indicates that the estimators are close together like in a cluster. [@Gudivada17]

![*Figure 1*](images/Gudivada.jpg){fig-align="center" width="310"}

## Regularization

Regularization was introduced in the context of dealing with the matrix inverse. This inverse issue caused the answer to live outside of the needed mathematical space. The introduction of the regularization parameter is a smoothness penalty which allowed the problem to be solved. [@Bickel06]

The fitting of models with large numbers of parameters also has similar issues in which makes the models unstable. This issue requires the need for regularization to get a sensible model. The LASSO regression also has a penalty to ensure the model is stable and thus works with the data. [@Bickel06]

Ordinary Least Squares needed improvements due to the issue with large variance even though the process has a low bias. Shrinking or setting the coefficients to zero can improve the accuracy of the predictions. This process can introduce some bias but will reduce the variance which can improve the prediction accuracy. [@Hastie15]

### LASSO

LASSO or Least Absolute Shrinkage and Selection Operator is a regularized regression modeling method that performs variable selection and regularization. These two components allow for better accuracy in prediction and interpretation in the model. [@Dehmer19].

In the variable selection side, LASSO regression identifies the proper variables that will minimize prediction error and lessen the computer-intensive nature. Less variables, less computational power is needed. [@Cook18]

The selection of variables comes from a constraint on the model parameters. This is done by forcing the sum of the absolute value of these coefficients to be less than a fixed value $\lambda$. This constraint lowers the complexity of the model by eliminating variables from the model that are reduced to zero after the shrinkage. [@Cook18]

This shrinking of some of the coefficients to zero provides for an automated way for doing model selection in linear regression. [@Hastie15]

$\lambda$ is chosen using an automated k-fold cross-validation approach. This approach is were the data is partitioned into sub-samples of equal size. *k-1* out of *k* sub-samples are used for developing the model with the $k^{th}$ sample used to validate the model. This process is done *k* times to ensure each sub-set *k* is used to validate the model as some point. This process provides a range of values for $\lambda$. This provides a data set that allows for choosing a preferred $\lambda$. [@Cook18]

LASSO is not perfect. LASSO trades off the potential bias of individual parameter estimates for a better overall prediction. An important disadvantage to note is that the individual regression coefficients may be less reliable to interpret individually. The focus of the LASSO regression is to provide the best combined prediction. [@Cook18]

### Ridge Regression

Ridge Regression is a regularized regression model that predates the LASSO. [@Hastie15] This method also has a shrinking effect on the coefficients of the linear model but does not reduce these coefficients to zero. [@Dehmer19]. This method can simplify the problem from the original set of coefficients it does not have the added property of automated model selection.

### Elastic Net Regularization

Elastic Net Regularization is a regularized regression method that combines aspects from LASSO and ridge regressions. It does this by including a L2 norm penalty term in addition to the LASSO's L1 norm penalty term. Similarly to the LASSO, it attempts to improve upon linear regression's predictive accuracy along with the model's interpretability in the presence of many predictors. Specifically, elastic net regularization attempts to improve upon LASSO in three scenarios:

-   The case where there are more predictors than observations (p \> n). In this case, LASSO will predict at most n predictors.
-   Grouped variables with high pairwise correlations will cause LASSO to pick only one of the variables.
-   In the case of high multicollinearity in traditional regression scenarios (n \< p), LASSO's performance is weaker than ridge regression. This means that the model does not benefit from LASSO's feature selection properties.

The Elastic Net combines qualities from LASSO and Ridge Regression to create a method that deals well with multicollinearity while also employing LASSO-like feature selection. [@Hastie05]

## Why Utilize Regularized Regression Methods?

Regularized Regression Methods seek generally to improve the Ordinary Least Squares estimates in the regression problem. The two general areas that these methods attempt to improve are prediction accuracy in the model and model interpretability. Prediction accuracy concerns come from Ordinary Least Squares estimates tending to be overfit (having low bias and high variance). Several underlying issues can cause overfitting, including having too many parameters in the model, but the end result is that the model will not have high prediction accuracy.

To try and improve the prediction accuracy issue in these sorts of models, the regularization techniques try to alter the parameters (or their coefficients) in order to remove the high variance. This is often done at the expense of bias. The three regularization techniques handle this in different ways:

-   Ridge Regression: As stated above, this method shrinks the coefficients in the model according to a shrinkage penalty term based on the L2 norm and the sum of squared residuals (RSS). The coefficients that have the least influence in the model tend towards 0 the fastest. This reduces their impact on the model and typically lowers the variance, especially when multicollinearity is present.
-   LASSO Regression: This method uses a penalty term based on the L1 norm and the RSS to shrink, and even eliminate, coefficients from the model. As coefficients shrink, their impact on the model is reduced, but when a coefficient is dropped completely, the model simplifies further, helping to improve overfitting.\
-   Elastic Net Regularization: This is an extension of LASSO where an additional L2 norm penalty term is added to the LASSO. This serves to give LASSO qualities of Ridge regression and helps to improve prediction accuracy by shrinking coefficients and removing unimportant predictors.

The other main area that regularized regression methods attempt to improve is model interpretability. In this case, the LASSO and Elastic Net Regularization methods improve model interpretability by removing unimportant predictors from the model. This is also called feature selection. A downside of Ridge Regression is that it does not perform feature selection and is unable to improve model interpretability since all predictors remain in the model. In cases where there are high numbers of predictors, Ridge Regression, then, can produce a model that has good predictive ability, but poor interpretability. [@Hastie05] [@Tibshirani96]
