---
title: "LASSO Expeiment 1 AW"
editor: visual
---

```{r}

#Code based on tutorial from David Caughlin's LASSO Regression in R
#https://youtu.be/5GZ5BHOugBQ

#Import libraries 
library(GGally)
library(haven)
library(dbplyr)
library(dtplyr)
library(tidyverse)
library(readr)
library(caret)
library(randomForest)
library(mice)
library(gdata)
library(ggplot2)

```

```{r}

#Import data sets
analysis1 <- read_dta("analysis1.dta")
#View(analysis1)
df <- (analysis1)
#df <- as.matrix(analysis1)
df1 <- remove.vars(df, names=c("subjid","sex", "fmlyinc", "menopause","hip","hrtmedsselfever","hrtmedsself", "eselectin","pselectin", "reninria","creatinineu24hr", "albuminu24hr","dialysisduration", "publicinstype"), info=TRUE)

#analysis2 <- read_dta("G:/My Drive/STA6257/datasets/analysis2.dta")
#View(analysis2)

#analysis3 <- read_dta("G:/My Drive/STA6257/datasets/analysis3.dta")
#View(analysis3)
```

```{r}

#Using Ejection Fraction (ef) as Outcome Variable for Intial Experiment

#df <- as.vector.data.frame(analysis1)
#df <- as.matrix(analysis1)
#df <- remove.vars(df, names=c("subjid","sex", "fmlyinc", "menopause","hip","hrtmedsselfever","hrtmedsself", "eselectin","pselectin", "reninria","creatinineu24hr", "albuminu24hr","dialysisduration", "publicinstype"), info=TRUE)

#Dropped these variables to make analysis easier for this assignment. Variables had most NA's or non-numeric values.

#------------------------------------------------------------------------------


#THIS STEP TAKES 4EVER-ONLY RUN THIS ONCE TO GET "data_imp" THEN COMMENT OUT


#data_imp <- complete(mice(df1,m = 1,method = "pmm"))


# Predictive mean matching imputation


#--------------------------------------------------------------------------------


#OLD atempts
#df_imp <- md.pattern(df1)
#df$ef <- na.omit(df$ef)
#df$ef <- as.factor(df$ef) 



#fit_rf<-randomForest(as.factor(ef) ~ .,
        #data=as.matrix(df1),
        #importance=TRUE,
        #prOximity=TRUE,
        #na.action=na.roughfix)

#summary(analysis1)

```

```{r}
#Partitioning Data 80/20 Split
set.seed(123)

#Create Index Matrix: 80% split matrix NOT list only split once
index <- createDataPartition(data_imp$ef, p=0.8, list=FALSE, times=1)

#Create Test and Training df
#-is all except index
train_df <- data_imp[index,]
test_df <- data_imp[-index,]
```

```{r}
# k-fold Cross Validation to train LASSO
#(because sample size is large, using 10-fold)

#Train Control Function from Caret Package
#Create Object to assign all the training method info to 
#cross validation method, 10 fold
tctrl_method <- trainControl(method='cv', number=10,
                           savePredictions = 'all')

#Specify & train LASSO Regression Model
#Create vector of Lambda Values to find optimal (LASSO Tuning Parameter)
lambda_vector <- 10^seq(5,-5, length=500)

set.seed(123)

#LASSO Regression Model estimated from Training data and 10-fold cv
# dot is all other variables except outcome variable
#grand mean center, "center" and standardize, "scale" at this step
#c=combine, glmnet in caret package, alpha=1 for lasso (0 for ridge)




```

```{r}

#LASSO Model (alpha=1)

mod1 <- train(ef ~ .,
              data=train_df,
              preProcess=c("center", "scale"),
              method="glmnet",
              tuneGrid=expand.grid(alpha=1,lambda=lambda_vector),
              trControl=tctrl_method,
              na.action=na.omit)

#Warning Message OK!!


```

```{r}

#Best Optimal Lambda
mod1$bestTune
mod1$bestTune$lambda


#LASSO regression model coefficients/parameter estimates
coef(mod1$finalModel,mod1$bestTune$lambda)

#returns sparse matrix of 184 variables! 


#plot log(lambda) & RMSE 

plot(log(mod1$results$lambda),
     mod1$results$RMSE,
     xlab="log(lambda)",
     ylab="RMSE",
     xlim=c(-5,2))
     
log(0.03213764)



```

```{r}
#Predictor Variable Importance list top 20

varImp(mod1)


#Data Visualization of importance

ggplot(varImp(mod1))

```

```{r}
#Model Prediction on test data

predict1 <- predict(mod1, newdata=test_df)


#Model Accuracy

mod1_rmse <- data.frame(RMSE=RMSE(predict1, test_df$ef))

#RMSE 10


#R^2E

rss <- sum((predict1 - test_df$ef) ^ 2)
tss <- sum((test_df$ef - mean(test_df$ef)) ^ 2)
mod1_rsq <- 1 - rss/tss

mod1_rmse
mod1_rsq

#UHOH get Negative? R^2??




##OLD attempts 
#mod2acc <- data.frame(R2E=(predict1, test_df$ef))

#R SQUARED error metric 
#RSQUARE = function(y_actual,y_predict){
  #cor(y_actual,y_predict)^2
#}

#RSQUARE = function(predict1,test_df[ef]){
  #cor(predict1,test_df[ef])^2
#}

#mod2acc <- data.frame(Rsquared=R2(predict1,test_df$ef,use="pairwise.complete.obs"))
                      
#mod2acc <- data.frame(Rsquared=R2(as.numeric(predict1, test_df$ef,use="complete.obs")))
```

```{r}
#Compare LASSO to Ridge Regression

#Ridge Model (alpha=0)

mod2 <- train(ef ~ .,
              data=train_df,
              preProcess=c("center", "scale"),
              method="glmnet",
              tuneGrid=expand.grid(alpha=0,lambda=lambda_vector),
              trControl=tctrl_method,
              na.action=na.omit)

#Warning Message OK!!

```

```{r}
#RIDGE plot log(lambda) & RMSE 

plot(log(mod2$results$lambda),
     mod2$results$RMSE,
     xlab="log(lambda)",
     ylab="RMSE",
     xlim=c(-5,8))
     
log(0.03213764)



#Predictor Variable Importance list top 20

varImp(mod2)


#Data Visualization of importance

ggplot(varImp(mod2))


#Model Prediction on test data

predict2 <- predict(mod2, newdata=test_df)


#Model Accuracy

mod2_rmse <- data.frame(RMSE=RMSE(predict2, test_df$ef))

#RMSE 10


#R^2E

rss2 <- sum((predict2 - test_df$ef) ^ 2)
tss2 <- sum((test_df$ef - mean(test_df$ef)) ^ 2)
mod2_rsq <- 1 - rss2/tss2

mod2_rmse
mod2_rsq


```

```{r}
#ElasticNet Regresstion

#tctrl_method (using from LASSO earlier)

#Set Seed (reproducible results)
set.seed(123)

#Specify OLS model estimated with training data, train_df
#10-fold cross validation

mod3 <- train(ef ~ .,
              data=train_df,
              preProcess=c("center", "scale"),
              method="glmnet",
              tuneGrid=expand.grid(alpha=0.5,lambda=lambda_vector),
              trControl=tctrl_method,
              na.action=na.omit)
#print(mod3)
```
Elastic Net
The final values used for the model were alpha = 0.5 and lambda = 0.1343558




```{r}
summary(mod3)
```





```{r}
#Predict outcome from training data based on test data using ElasticNet

predict3 <- predict(mod3, newdata=test_df)

#Assess model performance
#Model Accuracy

mod3_rmse <- data.frame(RMSE=RMSE(predict3, test_df$ef))

#RMSE 10


#R^2E

rss3 <- sum((predict3 - test_df$ef) ^ 2)
tss3 <- sum((test_df$ef - mean(test_df$ef)) ^ 2)
mod3_rsq <- 1 - rss3/tss3

mod3_rmse
mod3_rsq

print(mod3_rmse)
print(mod3_rsq)
```



```{r}
#Compare LASSO to OLS Regresstion

#tctrl_method (using from LASSO earlier)

#Set Seed (reproducible results)
set.seed(123)

#Specify OLS model estimated with training data, train_df
#10-fold cross validation

mod4 <- train(ef ~ .,
             data=train_df,
             preProcess=c("center","scale"),#grand means center and scale to make results comparable to LASSO
             method="lm",
             trControl=tctrl_method,
             na.action=na.omit)
#print(mod4)

```

```{r}
summary(mod4)
```

-   Note no coefficients are shrunk to zero for this and OLS and model contains multicollinearities

```{r}
#Compare LASSO, Ridge, ElasticNet, OLS

model_list <- list(mod1,mod2,mod3,mod4)
resample <- resamples(model_list)
summary(resample)
```

```{r}
#compare models 
compare_models(mod1,mod2, metric="RMSE")
compare_models(mod1,mod2, metric="Rsquared")

compare_models(mod1,mod3, metric="RMSE")
compare_models(mod1,mod3, metric="Rsquared")

compare_models(mod1,mod4, metric="RMSE")
compare_models(mod1,mod4, metric="Rsquared")


compare_models(mod2,mod3, metric="RMSE")
compare_models(mod2,mod3, metric="Rsquared")

compare_models(mod2,mod4, metric="RMSE")
compare_models(mod2,mod4, metric="Rsquared")

compare_models(mod3,mod4, metric="RMSE")
compare_models(mod3,mod4, metric="Rsquared")

```

```{r}
#Predict outcome from training data based on test data using OLS

predict4 <- predict(mod4, newdata=test_df)

#Assess model performance
#Model Accuracy

mod4_rmse <- data.frame(RMSE=RMSE(predict4, test_df$ef))

#RMSE 10


#R^2E

rss4 <- sum((predict4 - test_df$ef) ^ 2)
tss4 <- sum((test_df$ef - mean(test_df$ef)) ^ 2)
mod4_rsq <- 1 - rss4/tss4

mod4_rmse
mod4_rsq

print(mod4_rmse)
print(mod4_rsq)
```

```{r}
#Compare LASSO and OLS predictive performance based on test_df

comp <- matrix(c(mod1_rmse, mod1_rsq,
                 mod2_rmse,mod2_rsq,
                 mod3_rmse,mod3_rsq,
                 mod4_rmse,mod4_rsq),
               ncol=2,byrow=TRUE)

#Labels
colnames(comp) <- c("RMSE","R-squared")
rownames(comp) <- c("LASSO", "Ridge", "ElasticNet", "OLS")


```
