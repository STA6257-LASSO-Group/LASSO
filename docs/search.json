[
  {
    "objectID": "HumanFreedomIndex.html",
    "href": "HumanFreedomIndex.html",
    "title": "6  Human Freedom Index (High Dimensional Data Example)",
    "section": "",
    "text": "The techniques in this demonstration utilize the glmnet package to fit the regularized models. More information about this package can be found at (“An Introduction to ‘Glmnet‘” 2021)\nFor this demonstration, the political freedom homicide variable will be the response variable. Each of the other distinct variables (excluding indexes and total scores) will be used as predictors.\n\n\n\n\n6.0.1 Trimming and Fitting a Linear Model\n\n#Human Freedom Index Linear Model\nfreedomData <- hfi\nfreedomData <- freedomData %>% filter(year==2016)%>% select(c(-\"year\",-\"ISO_code\",-\"countries\",-\"region\",-\"ef_score\",-\"ef_rank\",-\"hf_rank\",-\"hf_quartile\",-\"pf_score\",-\"pf_rank\",-\"pf_religion_estop_establish\",-\"pf_religion_estop_operate\",-\"pf_identity_legal\",-\"pf_rol_procedural\",-\"pf_rol_civil\",-\"pf_rol_criminal\",-\"pf_ss_women_inheritance_widows\",-\"pf_ss_women_inheritance_daughters\",-\"pf_association_political_establish\",-\"pf_association_political_operate\",-\"pf_association_sport_operate\",-\"pf_association_sport_establish\",-\"pf_identity_divorce\",-\"pf_association_prof_operate\",-\"pf_association_prof_establish\"))%>%mutate(id = row_number())\n#check missing data values\nfreedomData <- na.omit(freedomData) \ntrain <- freedomData %>% sample_frac(.8)\ntest <- anti_join(freedomData, train, by='id')\n\nytrain <- train$pf_ss_homicide\nytest <- test$pf_ss_homicide\nxtrainLin <- train %>% select(c(-\"id\"))\nxtrain <- train %>% select(c(-\"pf_ss_homicide\",-\"id\"))\nxtestLin <- test %>% select(c(-\"id\"))\nxtest <- test %>% select(c(-\"pf_ss_homicide\",-\"id\"))\n\nxtestFrame <- xtest\nxtest <- data.matrix(xtest)\npredictors <- data.matrix(xtrain)\nresp <- ytrain\n\nlinModel <- lm(hf_score~., data=xtrainLin)\nlinSummary <- summary(linModel)\n\nThe Linear Model has an adjusted R-Squared of NaN. In addition, it doesn’t have any residual degrees of freedom, and invalid test scores. The model is not workable due to a lack of degrees of freedom to fit the number of predictors in the data set. In this case, the number of predictors could be trimmed at random to get a working model, but the regularization techniques can be used to evaluate this data set without randomly choosing predictors.\nTo fit the regularization models, the glmnet function will be used. The CV.glmnet function below performs a cross-fold validation on the training data in order to obtain the optimal lambda value for the respective model. In this function, the value of alpha controls the form of regularization that will be applied. An alpha value of 1 corresponds to LASSO regression, a value of 0 corresponds to Ridge Regression, and a value in between corresponds to a form of Elastic Net Regularization. The magnitude of alpha controls the penalty term in the Elastic Net Regularization. As in most regularization problems, the value of the penalty term (in this case lambda) is of critical importance.\n\n\n6.0.2 Fitting Regularized Models\nEach method will have a model fit. These will then have their RSquare and RMSE scores displayed to show a relative performance for each method.\nThe basic code structure is as follows:\n\nPerform Cross Validation to acquire the optimal lambda. The alpha term is altered based on the method.\nPass the optimal lambda into a new model based on the training data.\nExamine the impact on the coefficients.\nUsing the model, make predictions on the training data and the test data.\nStore results and display scores for each method.\n\n\n#LASSO\nmodelResults <- data.frame(matrix(ncol=6,nrow=0))\ncolnames(modelResults)<-c(\"Model\",\"Train_RSquare\",\"Train_RMSE\",\"Test_RSquare\",\"Test_RMSE\",\"CoefficientCount\")\n\nmodel <- cv.glmnet(predictors, resp, alpha=1)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\nplot(model)\n\n\n\n\nThe LASSO Lambda plot demonstrates the MSE for different values of lambda.\n\nfinalModel <- glmnet(predictors,resp, alpha=1, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"LASSO\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\nLASSOCoef <- coefList\n\n\n#Ridge\nmodel <- cv.glmnet(predictors, resp, alpha=0)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\nplot(model)\n\n\n\n\nThe Ridge Regression plot shows higher values of lambda. For Ridge Regression, the choice of lambda represents a factor of shrinkage. A lambda of 0 would be equivalent to normal linear regression while a very high lambda would shrink all coefficients towards 0. Now the model is fit with the optimal lambda. The LASSO process is repeated to determine the coefficients which can be seen in the collapsed code below.\n\nfinalModel <- glmnet(predictors,resp, alpha=0, lambda=bestLambda)\n\n\n\nShow the code\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"Ridge\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\n\n\nThe process is now repeated for three different elastic net models fit with different increments of alpha. You can display the code using the option below.\n\n\nShow the code\n#Elastic Net 1\nmodel <- cv.glmnet(predictors, resp, alpha=.25)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\n\n\nfinalModel <- glmnet(predictors,resp, alpha=.25, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"ENet.25\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\n\n\n#Elastic Net 2\nmodel <- cv.glmnet(predictors, resp, alpha=.5)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\n\n\nfinalModel <- glmnet(predictors,resp, alpha=.5, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"ENet.50\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\n\n#Elastic Net 3\nmodel <- cv.glmnet(predictors, resp, alpha=.75)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\n\n\nfinalModel <- glmnet(predictors,resp, alpha=.75, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"ENet.75\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\n\n\n\n\n6.0.3 Regularization Results / Discussion\nThe final model group looks like the table below.\n\n\n    Model Train_RSquare Train_RMSE Test_RSquare  Test_RMSE CoefficientCount\n1   LASSO     0.9990647 0.07949565    0.9976974 0.02945082               10\n2   Ridge     0.5262948 1.78901121    0.2828350 0.51975496               98\n3 ENet.25     0.9904047 0.25461763    0.8546090 0.23402265               41\n4 ENet.50     0.9968741 0.14532635    0.9747321 0.09756052               27\n5 ENet.75     0.9987773 0.09089239    0.9943804 0.04600908               13\n\n\nThe number of coefficients selected by each individual method corresponds to how “LASSO” like or “Ridge” like the alpha parameter was set. The pure LASSO regression shows that 9 predictors (without the intercept term) were selected, while the pure Ridge Regression shows all 98 possible predictors were chosen. The Elastic Net models fall somewhere between, depending on the strength of the alpha parameter.\nThe LASSO selected features included:\n\n\n                         Predictor           Coefficient\n1                      (Intercept)     0.146490919100984\n2  pf_ss_disappearances_fatalities   -0.0212363595160555\n3             pf_ss_disappearances    -0.913737552722075\n4                      pf_ss_women    -0.955047621110517\n5                            pf_ss      2.85946335709078\n6           pf_religion_harassment   -0.0127804254185799\n7                      pf_religion -0.000370056255707164\n8           pf_expression_internet   -0.0013295634783857\n9                   ef_legal_crime    0.0390520848618338\n10         ef_trade_movement_visit  -0.00249144481920272\n\n\nThe model with the best fit in this case was the pure LASSO model. As has been shown, this regression model can be used for predictions. More refinement can be performed. For example, in this case, there is still the likelihood that the model is overfit since 17 predictors is still a large number when compared to 93 observations. To further understand this is a matter of the underlying mechanics in the data. It is also possible that these 17 predictors still have some manner of collinearity present. Regularization, in this case, has now given a workable group of features to analyze for a refined linear model.\n\n\n\n\n“An Introduction to ‘Glmnet‘.” 2021. https://glmnet.stanford.edu/articles/glmnet.html."
  },
  {
    "objectID": "LiteratureReview.html",
    "href": "LiteratureReview.html",
    "title": "2  Current Literature",
    "section": "",
    "text": "Regularized Regression techniques have been used extensively in the available literature, and, interestingly, the application is not strictly limited to the regression problem. For example, the feature selection qualities of the LASSO method have been useful as a preprocessor before applying a machine learning method such as a neural network.\nBecause these methods are well established, there is continuing development on extensions of them in the literature. For example, (Sethi and Mittal 2021) used a method based on LASSO called Correlation based Adaptive LASSO to predict air quality across areas of Delhi, India. This method is actually an “extension of an extension” since Adaptive LASSO is itself based on the original LASSO method. Adaptive LASSO is a LASSO method with oracle properties that utilizes Ridge Regression to help prevent the overfitting of large coefficients that can occur with traditional LASSO. This example demonstrates that regularization methods are a changing field of study."
  },
  {
    "objectID": "LiteratureReview.html#feature-selection",
    "href": "LiteratureReview.html#feature-selection",
    "title": "2  Current Literature",
    "section": "2.2 Feature selection",
    "text": "2.2 Feature selection\nLASSO, Ridge, and Elastic Net Regression simplify the features by reducing and/or simplifying the values to zero. Due to this type of feature selection and/or reduction, the model may result in poor fitting. (Aheto et al. 2021) This ill-fitting is mentioned in the previous section on regression line fitting.\nModeling a situation where the number of features is larger than the number of observations is a difficult and unsolved problem in many situations. (Freijeiro-González, Febrero-Bande, and González-Manteiga 2020)\nIn their paper, (Song and Pan 2021) used a LASSO regression algorithm to perform feature selection on a time-series problem that studied gas concentration rates over time in Chinese coal mines. The idea behind this study was that the concentration of gas is linked to accident rates in coal mines, but there were a large number of features to deal with. The researchers used LASSO to perform feature selection before fitting an ANN with both the complete set and reduced set of features. In this case, the predictors solved the LASSO problem (finding the optimal value of the shrinkage penalty Lambda) using the Least Angle Regression (LARS) algorithm.\n(Aheto et al. 2021) used Regularized Regression to study the prevalence of child malaria in Ghana. In their study, the different dependent variables were unable to be well modeled by other approaches (such as ARIMA and correlation models), so regularization was applied to reduce the number of covariates in the models."
  },
  {
    "objectID": "LiteratureReview.html#high-dimensional-data",
    "href": "LiteratureReview.html#high-dimensional-data",
    "title": "2  Current Literature",
    "section": "2.3 High Dimensional Data",
    "text": "2.3 High Dimensional Data\nRegularized regression methods are frequently used in data sets with high dimension. These sorts of data sets can be frequently found in data studying the effects of genes since genomic data is frequently very high dimension relative to observations. In their paper (Ogutu, Schulz-Streeck, and Piepho 2012), the researchers compare the performance of LASSO, Ridge, and Elastic Net Regularization to try and determine the best markers in the genome that are related to Genomic Breeding Value.\n\n\n\n\nAheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua. 2021. “A Predictive Model, and Predictors of Under-Five Child Malaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net Regression Approaches Compare?” Preventive Medicine Reports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\nFreijeiro-González, Laura, Manuel Febrero-Bande, and Wenceslao González-Manteiga. 2020. “A critical review of LASSO and its derivatives for variable selection under dependence among covariates.” arXiv e-Prints, December, arXiv:2012.11470. https://arxiv.org/abs/2012.11470.\n\n\nOgutu, Joseph O., Torben Schulz-Streeck, and Hans-Peter Piepho. 2012. “Genomic Selection Using Regularized Linear Regression Models: Ridge Regression, Lasso, Elastic Net and Their Extensions.” BMC Proceedings 6. https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/genomic-selection-using-regularized-linear/docview/1014440955/se-2.\n\n\nSethi, Jasleen Kaur, and Mamta Mittal. 2021. “An Efficient Correlation Based Adaptive LASSO Regression Method for Air Quality Index Prediction.” Earth Science Informatics 14 (4): 1777–86. https://doi.org/10.1007/s12145-021-00618-1.\n\n\nSong, Li, Shuang, and Shaobo Pan. 2021. “Research on Time Series Characteristics of the Gas Drainage Evaluation Index Based on Lasso Regression.” Scientific Reports 11 (1). https://doi.org/10.1038/s41598-021-00210-z."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Regression is the predicting or learning of numerical features in statistics and machine learning. This process includes the prediction of numeric as well as categorical attributes. Performing a regression allows for the determination of which factors in situations matter, what is their impact, and how they influence other factors. Regression allows for data driven decisions that eliminate guesswork. (Joshi and Saxena 2020)\nSimple linear regression finds the relationship between a dependent and independent variable whereas multiple linear regression can find the relationship between the dependent and multiple independent variables. (Joshi and Saxena 2020)\nThe data is modeled on a graph and a line is fitted to the data. This allows for the calculation of the coefficients of the independent variable or variables. (Joshi and Saxena 2020)\n\n\nRegression line fitting is a delicate balance as the line should predict the desired value well based on several features but be applicable enough to be used repeatedly with different samples of data. The line should not be under-fitted, which would not provide an accurate prediction. The line also should not be over-fitted to the sample, which would cause the model not to work well with different samples and the data at large. (Jabbar and Khan 2014)\nSeveral methods can be used to ensure appropriate line fitting. We can compare the data to the AUC Curves which determining the curves predictive ability by quantifying with the Area Under the Curve. The higher the AUC the more predictive the model. The data is separated into two parts. 80% of the data is labeled as the training or estimation data and 20% of the data is labeled as the validation set. Both parts of the data are tested to ensure the AUC values are similar.(Aheto et al. 2021)\nThe penalty method is important to prevent over-fitting. We set \\(E_{train}\\) and \\(E_{test}\\) to be the training set error and test error, respectively. We then try to find a the minimal penalty such that \\(E_{test} = E_{train} + Penalty\\) (Jabbar and Khan 2014)\nThe early stopping method is used for the prevention of both over and under-fitting. The sample data is broken down into three parts: training, validation, and testing. This data is then broken down in to the similar 80%/20% split to estimate the line and then to validate the process. (Jabbar and Khan 2014)\n\n\n\nBias measures the amount of deviation from the expected value is from the estimator in the models. Variance measures how far the data fluctuates. (Mehta, Bukov, Wang)\nIf we consider the bulls eye as an example: low bias indicates that the estimators are in the vicinity of the bulls eye and low variance indicates that the estimators are close together like in a cluster. (Gudivada 2017)\n\n\n\nGudivada, Venkat & Apon, Amy & Ding, Junhua. (2017). Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations. International Journal on Advances in Software. Figure 1"
  },
  {
    "objectID": "intro.html#regularization",
    "href": "intro.html#regularization",
    "title": "1  Introduction",
    "section": "1.2 Regularization",
    "text": "1.2 Regularization\nRegularization was introduced in the context of dealing with the matrix inverse. This inverse issue caused the answer to live outside of the needed mathematical space. The introduction of the regularization parameter is a smoothness penalty which allowed the problem to be solved. (Fan et al. 2006)\nThe fitting of models with large numbers of parameters also has similar issues in which makes the models unstable. This issue requires the need for regularization to get a sensible model. The LASSO regression also has a penalty to ensure the model is stable and thus works with the data. (Fan et al. 2006)\nOrdinary least squares needed improvements due to the issue with large variance even though the process has a low bias. Shrinking or setting the coefficients to zero can improve the accuracy of the predictions. This process can introduce some bias but will reduce the variance which can improve the prediction accuracy. (Hastie and Tibshirani 2015)\n\n1.2.1 LASSO\nLASSO or Least Absolute Shrinkage and Selection Operator is a regularized regression modeling method that performs variable selection and regularization. These two components allow for better accuracy in prediction and interpretation in the model. (Emmert-Streib and Dehmer 2019).\nIn the variable selection side, LASSO regression identifies the proper variables that will minimize prediction error and lessen the computer-intensive nature. Less variables mean less computational power is needed. (Ranstam and Cook 2018)\nThe selection of variables comes from a constraint on the model parameters. This is done by forcing the sum of the absolute value of these coefficients to be less than a fixed value \\(\\lambda\\). This constraint lowers the complexity of the model by eliminating variables from the model that are reduced to zero after the shrinkage. (Ranstam and Cook 2018)\nThis shrinking of some of the coefficients to zero provides for an automated way for doing model selection in linear regression. (Hastie and Tibshirani 2015)\n\\(\\lambda\\) is chosen using an automated k-fold cross-validation approach. This approach is were the data is partitioned into sub-samples of equal size. k-1 out of k sub-samples are used for developing the model with the \\(k^{th}\\) sample used to validate the model. This process is done k times to ensure each sub-set k is used to validate the model as some point. This process provides a range of values for \\(\\lambda\\) and provides a data set to determine a preferred \\(\\lambda\\). (Ranstam and Cook 2018)\nLASSO is not perfect. LASSO trades off the potential bias of individual parameter estimates for a better overall prediction. (Ranstam, Cook). This method allows for the determination of a smaller subset of predictors that shows the strongest effects on the model. (Hastie and Tibshirani 2015)Important disadvantage to note is that the individual regression coefficients may be less reliable to interpret individually (Ranstam and Cook 2018). Also, LASSO is not capable of selecting more predictors than the number of of observations in the sample.(Emmert-Streib and Dehmer 2019).\nThe focus of the LASSO regression is to provide the best overall prediction and to reduce the number of variables to identify the predictors of importance not to interpret the meaning of individual predictor coefficients or keep excessive predictors (Ranstam and Cook 2018).\n\n\n1.2.2 Ridge Regression\nRidge Regression is a regularized regression model that predates LASSO (Hastie and Tibshirani 2015). Again, the need for improving the ordinary least squares comes from the fact that these models have a low bias but large variance. We can trade off some of the large variance by introducing some estimation bias allowing for accuracy of the prediction to improve. The preferred \\(\\lambda\\) in this model is different from the LASSO \\(\\lambda\\) as this ridge parameter reduces the coefficients toward zero without eliminating the the variables completely by reducing coefficients to zero.(Emmert-Streib and Dehmer 2019)\nRidge Regression works best when the ordinary least squares estimates have a high variance and the number of predictors is larger than the number of samples. The benefit of this method is the trade off of reducing the variance by increasing a low bias which can improve the prediction accuracy of the model.(Emmert-Streib and Dehmer 2019)\nThe downfall with this method is the reduction of the coefficients toward zero not to zero. Ridge regression therefore does not provide variable selection as the LASSO method. (Emmert-Streib and Dehmer 2019)\n\n\n1.2.3 Elastic Net Regularization\nThe elastic net was designed to improve the LASSO method as well as combine the benefits of LASSO and ridge regression. The elastic net has the benefit of LASSO regression as this method can perform variable selection and the benefit of ridge regression since it can be used in cases where the number of predictors is larger than the number of observations. Recall that ridge regression cannot perform variable selection and LASSO cannot manage cases with the number of predictors is much larger than the number of observations.(Emmert-Streib and Dehmer 2019)\nAn issue that can arise from the LASSO is in situations with strongly correlated predictors the variable selection method keeps only the strongest variable. Whereas, the elastic net tends to keep these strongly correlated predictors together in the model with its underlying grouping technique. (Emmert-Streib and Dehmer 2019)\nThe elastic net is able to blend the methods of LASSO and ridge regression by combining the squared penalty both methods and weighing them based on the number of correlated predictors. (Hastie and Tibshirani 2015)\nIt does this by including a L2 norm penalty term in addition to the LASSO’s L1 norm penalty term. Similarly to the LASSO, it attempts to improve upon linear regression’s predictive accuracy along with the model’s interpretability in the presence of many predictors. Specifically, elastic net regularization attempts to improve upon LASSO in three scenarios:\n\nThe case where there are more predictors than observations (p > n). In this case, LASSO will predict at most n predictors.\nGrouped variables with high pairwise correlations will cause LASSO to pick only one of the variables.\nIn the case of high multicollinearity in traditional regression scenarios (n < p), LASSO’s performance is weaker than ridge regression. This means that the model does not benefit from LASSO’s feature selection properties.\n\nThe Elastic Net combines qualities from LASSO and Ridge Regression to create a method that deals well with multicollinearity while also employing LASSO-like feature selection. (Zou and Hastie 2005)\n\n\n1.2.4 Comparison: Ridge, LASSO, and Elastic Net\nRidge regression contains the most variables as it does not participate in variable selection. The coefficients go toward zero but are not eliminated. The LASSO model contains the least number of variables as it participates in variable selection including the selection of only one variable in situations where a set of variables exhibit multicollinearity. The Elastic Net model can be weighted to lean more towards the ridge regression eliminating less variables as alpha is set closer to 0 or more towards the LASSO model eliminating more variables with an alpha is closer to 1. This alpha can also be computed using cross-validation. (Aheto et al. 2021)\nThe selection of variables is a highly favorable component when the number of parameters is large as the computational power need can be costly and time consuming. This may be a reason for leaning more so to the LASSO method as it selects the least number of parameters. The drawback is that LASSO is not recommended for when the number of predictors is larger than the number of observations (\\(p>n\\)) as LASSO can only select at most n predictors. Ridge does not have this restriction as it does not remove the predictors but reduces the coefficients to close to zero. Elastic Net also does not have this restriction but preforms variable selection. Elastic net is particularly useful in cases where the number of predictors is larger than the number of of observations ( \\(p>n\\)) or even if the number of predictors is much larger than the number of observations ( \\(p>>n\\)). (Emmert-Streib and Dehmer 2019)\n\n\n1.2.5 Comparison: Ridge, LASSO, and Elastic Net\nRidge regression contains the most variables as it does not participate in variable selection. The coefficients go toward zero but are not eliminated. The LASSO model contains the least number of variables as it participates in variable selection including the selection of only one variable in situations where a set of variables exhibit multicollinearity. The Elastic Net model can be weighted to lean more towards the ridge regression eliminating less variables as alpha is set closer to 0 or more towards the LASSO model eliminating more variables with an alpha is closer to 1. This alpha can also be computed using cross-validation. (Aheto et al. 2021)\nThe selection of variables is a highly favorable component when the number of parameters is large as the computational power need can be costly and time consuming. This may be a reason for leaning more so to the LASSO method as it selects the least number of parameters. The drawback is that LASSO is not recommended for when the number of predictors is larger than the number of observations (\\(p>n\\)) as LASSO can only select at most n predictors. Ridge does not have this restriction as it does not remove the predictors but reduces the coefficients to close to zero. Elastic Net also does not have this restriction but preforms variable selection. Elastic net is particularly useful in cases where the number of predictors is larger than the number of of observations ( \\(p>n\\)) or even if the number of predictors is much larger than the number of observations ( \\(p>>n\\)). (Emmert-Streib and Dehmer 2019)"
  },
  {
    "objectID": "intro.html#why-utilize-regularized-regression-methods",
    "href": "intro.html#why-utilize-regularized-regression-methods",
    "title": "1  Introduction",
    "section": "1.3 Why Utilize Regularized Regression Methods?",
    "text": "1.3 Why Utilize Regularized Regression Methods?\nRegularized Regression Methods seek generally to improve the Ordinary Least Squares estimates in the regression problem. The two general areas that these methods attempt to improve are prediction accuracy in the model and model interpretability. Prediction accuracy concerns come from Ordinary Least Squares estimates tending to be overfit (having low bias and high variance). Several underlying issues can cause overfitting, including having too many parameters in the model, but the end result is that the model will not have high prediction accuracy.\nTo try and improve the prediction accuracy issue in these sorts of models, the regularization techniques try to alter the parameters (or their coefficients) in order to remove the high variance. This is often done at the expense of bias. The three regularization techniques handle this in different ways:\n\nRidge Regression: As stated above, this method shrinks the coefficients in the model according to a shrinkage penalty term based on the L2 norm and the sum of squared residuals (RSS). The coefficients that have the least influence in the model tend towards 0 the fastest. This reduces their impact on the model and typically lowers the variance, especially when multicollinearity is present.\nLASSO Regression: This method uses a penalty term based on the L1 norm and the RSS to shrink, and even eliminate, coefficients from the model. As coefficients shrink, their impact on the model is reduced, but when a coefficient is dropped completely, the model simplifies further, helping to improve overfitting.\n\nElastic Net Regularization: This is an extension of LASSO where an additional L2 norm penalty term is added to the LASSO. This serves to give LASSO qualities of Ridge regression and helps to improve prediction accuracy by shrinking coefficients and removing unimportant predictors.\n\nThe other main area that regularized regression methods attempt to improve is model interpretability. In this case, the LASSO and Elastic Net Regularization methods improve model interpretability by removing unimportant predictors from the model. This is also called feature selection. A downside of Ridge Regression is that it does not perform feature selection and is unable to improve model interpretability since all predictors remain in the model. In cases where there are high numbers of predictors, Ridge Regression, then, can produce a model that has good predictive ability, but poor interpretability. (Zou and Hastie 2005) (Tibshirani 1996)\n\n\n\n\nAheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua. 2021. “A Predictive Model, and Predictors of Under-Five Child Malaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net Regression Approaches Compare?” Preventive Medicine Reports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\nEmmert-Streib, Frank, and Matthias Dehmer. 2019. “High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection.” Machine Learning and Knowledge Extraction 27 (2): 359–83. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nFan, Jianqing, Peter Bickel, Bo Li, Alexandre Tsybakov, Sara van de Geer, B. Yu, Teofilo Valdes, Carlos Rivero, and Aad Vaart. 2006. “Regularization in Statistics.” TEST: An Official Journal of the Spanish Society of Statistics and Operations Research 15 (February): 271–344. https://doi.org/10.1007/BF02607055.\n\n\nGudivada, Amy & Ding, Venkat & Apon. 2017. Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations. International Journal on Advances in Software.\n\n\nHastie, Wainwright, Trevor, and Robert Tibshirani. 2015. Statistical Learning with Sparsity: The LASSO and Generalizations. CRC Press, Taylor & Francis Group.\n\n\nJabbar, Haider Khalaf, and Rafiqul Zaman Khan. 2014. “Methods to Avoid over-Fitting and Under-Fitting in Supervised Machine Learning (Comparative Study).” In.\n\n\nJoshi, Jigyasu, and Shweta Saxena. 2020. “Regression Analysis in Data Science.” Journal of Analysis and Computation, May, 97–111.\n\n\nRanstam, J, and J A Cook. 2018. “LASSO regression.” British Journal of Surgery 105 (10): 1348–48. https://doi.org/10.1002/bjs.10895.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267–88. http://www.jstor.org/stable/2346178.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society. Series B (Statistical Methodology) 67 (2): 301–20. http://www.jstor.org/stable/3647580."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Overview of LASSO, Ridge, and Elastic Net Regularized Regression Methods",
    "section": "",
    "text": "This project will present an overview of three regularized regression methods: LASSO regression, Ridge Regression, and the Elastic Net.\nSlides\nSlides QMD"
  },
  {
    "objectID": "dataset_methods.html",
    "href": "dataset_methods.html",
    "title": "3  Methods and Materials",
    "section": "",
    "text": "3.0.2 Data sets\nPrevious studies indicate LASSO regression outperforms Ridge regression with sparse data sets in which the variable size is larger than the sample size. Conversely, Ridge regression has been shown to outperform LASSO regression with dense data in which the variable size is smaller than the sample size.(James et al. 2021) The hybrid of the two, Elastic Net regression, will adapt to either type of data by using the properties of LASSO and Ridge regression together. We aim to use two data sets with different characteristics to highlight the advantages of each of the three linear regularization techniques while also using these statistical methods to model the data and make predictions.\n\n\n3.0.3 Human Freedom Index\nThe Human Freedom Index from the open intro R package will be used to demonstrate regularization in a high dimensional example where the number of predictors is greater than the number of observations. This data consists of sociological measures of the different types of freedom. The main dataset has 1458 observations with 123 variables, but for this example, a single year of complete observations will be used. This results in a dataset of 93 observations across 99 variables and mimics the situation that would occur if a researcher attempted to analyze the most recent year of this data in isolation from the rest of the dataset.\nAside from year, country, and region identifiers, each column in this data set is a continuous numerical variable. Some variables have high degrees of collinearity because of their relation to one another. For example, total disappearances is expected to have high collinearity with total violent disappearances.\n\n\n3.0.4 Jackson Heart Study\nThe Jackson Heart Study (JHS) is an on-going community-based observational study. The data are collected from the medical histories, physical exams, and laboratory results of adult African Americans from three urban and rural counties in Jackson, Mississippi. There are 198 variables with an n of 2653 for the baseline exam Visit 1. After excluding redundant and categorical variables and imputing the data with MICE to account for missing values (see Programmatic Tools below) , we were left with 2653 observations and 27 variables (n>p). The JHS data was used to highlight Lasso regression’s performance over Ridge’s when the sample size greatly outnumbers the variable size. Additionally, we used this data set to test all three methods on shrinking multicollinearity and to test LASSO’s feature selection capabilities. Multicollinerity was measured by the Variance Inflation Factor (VIF). VIF is calculated by (1/1-R2) , where R2 is the sum of residuals squared divided by the total sum of squares of each individual x variable compared to all other x variables. A VIF score over 5 means that the variables are highly correlated. Tolerance is another way to measure multicollinearity and is the reciprocal of vif; high multicollinearity variables have a tolerance score above 0.2.\n\n\n3.0.5 Programmatic Tools, Statistical Methods, and Code Packages\nRStudio is the primary IDE used to run the demonstrations of these methods. The R Language is the coding language used. To deal with missing values in the multicollinearity example, the missing values are imputed using the MICE (Multiple Imputation by Chained Equations) algorithm. This algorithm is able to determine appropriate values for missing data by using the other variables in the dataset to converge upon a satisfactory value. Since collinearity is an important aspect of the regularization problem, correlation plots will be used to visually showcase collinearity among variables where appropriate. The ones being used take the form of heat-maps where bright red color indicates high positive correlation and dark blue color indicates high negative correlation. (“The Mice Algorithm,” n.d.) In addition, at the end of each demonstration, the R-squared and Root Mean Square Error will be calculated for the models run. The R-squared measures the among of predicted variable variance that is explained by the model while the error represents the standard deviation of the residuals. The Variance Inflation Factor (VIF) will be calculated in the multicollinearity example as it is a quantifier for how strong the multicollinearity effect is between variables. We will also be examining the difference between variable significance and importance where variable importance represents the amount of information a given predictor adds to the model and significance corresponds to whether or not a given predictor is explaining the random variation in a dependent variable. The primary code packages used to perform the Regularized Regression analyses will be two R packages: glmnet and caret. The glmnet package was developed at Stanford University by Trevor Hastie, Junyang Qian, and Kenneth Tay. It is designed to fit penalized generalized models and is able to calculate for the parameter lambda in an algorithmic approach (as opposed to having to provide a matrix of potential lambda variables). It is able to fit multiple regression situations, and, as such, has cross functional application beyond just the traditional linear LASSO regression implementation. In its arguments, this package can also be used to fit some of the extensions to LASSO such as relaxed LASSO. (“An Introduction to ‘Glmnet‘” 2021) Caret, by contrast, is a general classification and regression model package used in R. It is used for building models for a variety of problems, including generalized regression situations. In addition to regression problems, caret also contains methods for easily splitting test data, training models, and making predictions. (“A Short Introduction to the Caret Package,” n.d.) Additionally, we used the R package gamlr version 1.13-7 to perform the LASSO regression using a gamma distribution. The gamlr package is less widely used and documented than the glmnet package (“One-Step Estimator Paths for Concave Regularization,” n.d.). At this time, we were only able to run LASSO in gamlr as it’s features are not as customizable as the glmnet package. So, these results were taken for more of a qualitative analysis for the JHS dataset, and no comparisons could be made with Ridge and Elastic Net.\n\n\n\n\n“A Short Introduction to the Caret Package.” n.d. cran.r-project.org. https://cran.r-project.org/web/packages/caret/vignettes/caret.html.\n\n\n“An Introduction to ‘Glmnet‘.” 2021. https://glmnet.stanford.edu/articles/glmnet.html.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning, with Applications in r. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\n“One-Step Estimator Paths for Concave Regularization.” n.d. cran.r-project.org. http://arxiv.org/abs/1308.5623.\n\n\n“The Mice Algorithm.” n.d. cran.r-project.org. https://cran.r-project.org/web/packages/caret/vignettes/caret.html."
  },
  {
    "objectID": "dataset_methods.html#references",
    "href": "dataset_methods.html#references",
    "title": "3  Methods and Materials",
    "section": "3.2 References",
    "text": "3.2 References\nAshburner M, Ball CA, Blake JA, Botstein D, Butler H, Cherry JM, Davis AP, Dolinski K, Dwight SS, Eppig JT, Harris MA, Hill DP, Issel-Tarver L, Kasarskis A, Lewis S, Matese JC, Richardson JE, Ringwald M, Rubin GM, Sherlock G. Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet. 2000 May;25(1):25-9. doi: 10.1038/75556. PMID: 10802651; PMCID: PMC3037419.\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, https://www.statlearning.com, Springer-Verlag, New York"
  },
  {
    "objectID": "JacksonHeart.html",
    "href": "JacksonHeart.html",
    "title": "4  Generic Regularized Regression Demonstration (Jackson Heart Study Data)",
    "section": "",
    "text": "4.0.2 Ridge Regression Model\n\nmodel <- cv.glmnet(dfpred, dfResp, alpha=0)\nbestLambda <- model$lambda.min\nplot(model)\n\n\n\n\nThis plot shows the changing MSE for the various values of lambda. The algorithm chooses the lambda that gives the lowest MSE. The best lambda value chosen in this case was 3.9831346. This is now passed into the glmnet function to find the optimal model.\n\n#Passing the Optimal Lambda\nfinalModel <- glmnet(dfpred,dfResp, alpha=0, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = dfpred)\nrows<-nrow(modelResults)\n\nnewRow <- c(\"Ridge\",eval_results(dfResp,finalModelPredict,df)$Rsquare,eval_results(dfResp,finalModelPredict,df)$RMSE,count(coefList)-1)\nmodelResults[rows+1,]<-newRow\n\nprint(coefList)\n\n      Predictor         Coefficient\n1   (Intercept)    77.9871111372982\n2           age  -0.277007667349493\n3           bmi    1.33182793767548\n4           sbp   0.146659875361366\n5           dbp  -0.391297476907155\n6         hba1c    1.90906256347131\n7           fpg -0.0462856703071172\n8          alcw -0.0248444545737805\n9         waist   0.270243342420437\n10       weight   0.126556976636525\n11          bsa   -2.93499142457999\n12          abi   -11.2230610184193\n13       homa_b  0.0309288328108574\n14      totchol  0.0192024882091495\n15        trigs -0.0205898916075054\n16    fasthours -0.0438642861452437\n17       height  -0.549660555635953\n18 weeklystress  0.0144778624093354\n19   nbproblems   -1.54074622626403\n\n\nIn the Ridge regression, each variable has now had their coefficient scaled based on how much importance they add to the model. No variable can be eliminated using this method, but the penalty term can cause their coefficient, and therefore their weight, to approach 0.\n\n\n4.0.3 LASSO Regression Model\nIn the glmnet package, an alpha of 1 corresponds to a LASSO model.\n\nmodel <- cv.glmnet(dfpred, dfResp, alpha=1)\nbestLambda <- model$lambda.min\n\n\n#Passing the Optimal Lambda\nfinalModel <- glmnet(dfpred,dfResp, alpha=1, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = dfpred)\nrows<-nrow(modelResults)\n\nnewRow <- c(\"LASSO\",eval_results(dfResp,finalModelPredict,df)$Rsquare,eval_results(dfResp,finalModelPredict,df)$RMSE,count(coefList)-1)\nmodelResults[rows+1,]<-newRow\n\nprint(coefList)\n\n     Predictor         Coefficient\n1  (Intercept)    47.6326538855448\n2          bmi    2.14537255105611\n3          sbp  0.0585515721972467\n4          dbp   -0.25792785560877\n5        waist  0.0747155365397576\n6          abi   -8.16983430212925\n7       homa_b  0.0209504418095479\n8       height  -0.450987198636496\n9 weeklystress 0.00436079973519507\n\n\nIn the LASSO regression, the penalty term can cause the variables to be dropped from the model by setting their coefficient to 0. This occurs when their importance or the amount of information they add to the model is very low. In this case, several of the predictors have been dropped from the model.\n\n\n4.0.4 Elastic Net Model\nIn glmnet, an alpha between 0 and 1 will run elastic net regularization.\n\nmodel <- cv.glmnet(dfpred, dfResp, alpha=.25)\nbestLambda <- model$lambda.min\n#Passing the Optimal Lambda\nfinalModel <- glmnet(dfpred,dfResp, alpha=.25, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = dfpred)\nrows<-nrow(modelResults)\n\nnewRow <- c(\"Elastic Net\",eval_results(dfResp,finalModelPredict,df)$Rsquare,eval_results(dfResp,finalModelPredict,df)$RMSE,count(coefList)-1)\nmodelResults[rows+1,]<-newRow\n\nprint(coefList)\n\n      Predictor         Coefficient\n1   (Intercept)    60.8400168294724\n2           age  -0.162137469855504\n3           bmi    1.75627950976084\n4           sbp   0.115035773030254\n5           dbp  -0.339829555437511\n6         hba1c   0.591839690874828\n7         waist   0.220625478433008\n8           abi   -9.57342149887533\n9        homa_b  0.0281515728597371\n10      totchol 0.00627875606608306\n11        trigs -0.0104519264578124\n12       height  -0.507456715451998\n13 weeklystress 0.00903610772543083\n\n\nSimilarly to the LASSO regression, the Elastic Net method can also cause variables to be dropped from the model when they offer no importance to the prediction. The results of this example can be seen below.\n\n\n        Model     Train_RSquare       Train_RMSE Predictors\n1         OLS 0.581825663903433 12.6301455149939         18\n2       Ridge 0.592400531086241 12.4296512626905         18\n3       LASSO 0.570656480408171 12.7568836696496          8\n4 Elastic Net 0.587590241926988 12.5027804628744         12"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "The Human Freedom Index uses parameters to determine the personal and economic freedoms of a particular country or jurisdiction. The data used to create the model was trimmed to include the parameters that contained greater than 50% of responses then the countries and jurisdictions with complete observations was utilized. The response variable for the model is the political freedom homicide model. The remaining distinct variables will be used as predictors.\nFirst a linear model was attempted. This model returned an error regarding the degrees of freedom due to the number of predictors in the set. The natural response is the need for a reduction in the number of predictors. The best way to manage a reduction of parameters is the use of regularization models.\nThe three basic regularization models are ridge regression, elastic net, and LASSO. The data was then used to compute the LASSO \\(\\lambda\\) and the ridge regression \\(\\lambda\\). The elastic net uses both \\(\\lambda\\)’s as it is a blend of the two models.\nRidge regression reduces the parameters coefficients but does not eliminate any parameters, LASSO eliminates the most parameters, and the elastic net is a blend of the two. The blending of the elastic net can be determined by the selection of \\(\\alpha\\). Where an \\(\\alpha\\) closer to 0 is more like the ridge regression which retains more parameters and an \\(\\alpha\\) closer to 1 is closer to LASSO which retains less parameters.\nWe created 5 models.\n\n\n\nRegularization Model Type\nNumber of Coefficients\n\n\n\n\nRidge Regression\n98\n\n\nElastic net \\(\\alpha=0.25\\)\n41\n\n\nElastic net \\(\\alpha=0.50\\)\n27\n\n\nElastic net \\(\\alpha=0.75\\)\n13\n\n\nLASSO\n10\n\n\n\nThe data was broken down into an 80/20 split to allow for the creation and testing of the models. The R2 of the models were computed to determine the best model.\n\n\n\nRegularization Model Type\n\\(R^2\\)\n\n\n\n\nRidge Regression\n.283\n\n\nElastic net \\(\\alpha=0.25\\)\n.855\n\n\nElastic net \\(\\alpha=0.50\\)\n.975\n\n\nElastic net \\(\\alpha=0.75\\)\n.994\n\n\nLASSO\n.998\n\n\n\nThe pure LASSO model was determined to be the best fit. This model needs a little more testing to ensure the model is not overfit to the particular data. This may be an issue due to the low number of observations (\\(n=93\\)). This model did reduce the number of parameters methodically which can allow us to revisit the linear model."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua.\n2021. “A Predictive Model, and Predictors of Under-Five Child\nMalaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net\nRegression Approaches Compare?” Preventive Medicine\nReports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\n“An Introduction to ‘Glmnet‘.” 2021. https://glmnet.stanford.edu/articles/glmnet.html.\n\n\nEmmert-Streib, Frank, and Matthias Dehmer. 2019. “High-Dimensional\nLASSO-Based Computational Regression Models: Regularization, Shrinkage,\nand Selection.” Machine Learning and Knowledge\nExtraction 27 (2): 359–83. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nFan, Jianqing, Peter Bickel, Bo Li, Alexandre Tsybakov, Sara van de\nGeer, B. Yu, Teofilo Valdes, Carlos Rivero, and Aad Vaart. 2006.\n“Regularization in Statistics.” TEST: An Official\nJournal of the Spanish Society of Statistics and Operations\nResearch 15 (February): 271–344. https://doi.org/10.1007/BF02607055.\n\n\nFreijeiro-González, Laura, Manuel Febrero-Bande, and Wenceslao\nGonzález-Manteiga. 2020. “A critical review\nof LASSO and its derivatives for variable selection under dependence\namong covariates.” arXiv e-Prints, December,\narXiv:2012.11470. https://arxiv.org/abs/2012.11470.\n\n\nGudivada, Amy & Ding, Venkat & Apon. 2017. Data Quality\nConsiderations for Big Data and Machine Learning: Going Beyond Data\nCleaning and Transformations. International Journal on Advances in\nSoftware.\n\n\nHastie, Wainwright, Trevor, and Robert Tibshirani. 2015. Statistical\nLearning with Sparsity: The LASSO and Generalizations. CRC Press,\nTaylor & Francis Group.\n\n\nJabbar, Haider Khalaf, and Rafiqul Zaman Khan. 2014. “Methods to\nAvoid over-Fitting and Under-Fitting in Supervised Machine Learning\n(Comparative Study).” In.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning, with Applications in\nr. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nJoshi, Jigyasu, and Shweta Saxena. 2020. “Regression Analysis in\nData Science.” Journal of Analysis and Computation, May,\n97–111.\n\n\nOgutu, Joseph O., Torben Schulz-Streeck, and Hans-Peter Piepho. 2012.\n“Genomic Selection Using Regularized Linear Regression Models:\nRidge Regression, Lasso, Elastic Net and Their Extensions.”\nBMC Proceedings 6. https://login.ezproxy.lib.uwf.edu/login?url=https://www.proquest.com/scholarly-journals/genomic-selection-using-regularized-linear/docview/1014440955/se-2.\n\n\nRanstam, J, and J A Cook. 2018. “LASSO\nregression.” British Journal of Surgery 105 (10):\n1348–48. https://doi.org/10.1002/bjs.10895.\n\n\nSethi, Jasleen Kaur, and Mamta Mittal. 2021. “An Efficient\nCorrelation Based Adaptive LASSO Regression Method for Air Quality Index\nPrediction.” Earth Science Informatics 14 (4): 1777–86.\nhttps://doi.org/10.1007/s12145-021-00618-1.\n\n\nSong, Li, Shuang, and Shaobo Pan. 2021. “Research on Time Series\nCharacteristics of the Gas Drainage Evaluation Index Based on Lasso\nRegression.” Scientific Reports 11 (1). https://doi.org/10.1038/s41598-021-00210-z.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society. Series\nB (Methodological) 58 (1): 267–88. http://www.jstor.org/stable/2346178.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable\nSelection via the Elastic Net.” Journal of the Royal\nStatistical Society. Series B (Statistical Methodology) 67 (2):\n301–20. http://www.jstor.org/stable/3647580."
  },
  {
    "objectID": "LASSO_Experiment_11_20_AW.html",
    "href": "LASSO_Experiment_11_20_AW.html",
    "title": "5  Regularized Regression and the Jackson Heart Study Dataset: Multicollinearity",
    "section": "",
    "text": "We chose the Jackson Heart Study (JHS) dataset to demonstrate multicollinearity and create an environment in which Ridge Regression typically outperforms the LASSO method. When two or more variables are highly collinear, LASSO regression will select one randomly and drop the others to zero. Elastic Net adapts the properties of both LASSO and Ridge, so dropping coefficients is more difficult than LASSO, but can still occur. Whereas, Ridge regression will keep highly collinear variables and adjust their importance with weight. Although this technique introduces more variability in the coefficients and p-values, the prediction of the dependent variable is not affected. Ridge performs well when there are more observations than variables and does well with multicollinearity. Although we started by observing the full JHS Visit 1 dataset of 2653 observations and 198 variables with the correlation matrix, we simplified the final model used for the Regularization Regression methods to exclude non-numeric data and categorical data and to include variables from several “hot spots” from the larger correlation matrix shown below. It is important to note that these regularization regression techniques can be used with other methods such as logistic regression had we decided to analyze the categorical data as well. However, many of the categorical variables were, in fact, composites of or redundant variables to the continuous variables included in this study."
  },
  {
    "objectID": "LASSO_Experiment_11_20_AW.html#correlation-matrix-heat-map-suggests-multicollinearity-for-jackson-heart-study-visit-1",
    "href": "LASSO_Experiment_11_20_AW.html#correlation-matrix-heat-map-suggests-multicollinearity-for-jackson-heart-study-visit-1",
    "title": "5  Regularized Regression and the Jackson Heart Study Dataset: Multicollinearity",
    "section": "5.2 Correlation Matrix heat map suggests multicollinearity for Jackson Heart Study Visit 1",
    "text": "5.2 Correlation Matrix heat map suggests multicollinearity for Jackson Heart Study Visit 1\nVisit 1"
  },
  {
    "objectID": "LASSO_Experiment_11_20_AW.html#variance-inflation-factor-and-tolerance-scores-suggest-severe-multicollinearity-in-jackson-heart-study-visit-1",
    "href": "LASSO_Experiment_11_20_AW.html#variance-inflation-factor-and-tolerance-scores-suggest-severe-multicollinearity-in-jackson-heart-study-visit-1",
    "title": "5  Regularized Regression and the Jackson Heart Study Dataset: Multicollinearity",
    "section": "5.3 Variance Inflation Factor and Tolerance Scores suggest severe multicollinearity in Jackson Heart Study Visit 1",
    "text": "5.3 Variance Inflation Factor and Tolerance Scores suggest severe multicollinearity in Jackson Heart Study Visit 1\n\n\n5.3.0.1 VIF Function gives Multicollinearity error for Linear Model of Ejection Fraction\n\n\n\n5.3.1 Analysis of matrix “hot spots” demonstrates the LASSO’s feature selection capabilities and the Ridge’s ability to preserve a highly collinear model with competitive results\n\n\n\n\n\n\n\n                          VIF   Tolerance\nheight              31.358866 0.031888908\nage                  2.310040 0.432892905\nweight             104.244240 0.009592856\nbmi                 95.368793 0.010485610\nneck                 2.502269 0.399637366\nwaist                4.355093 0.229616199\nnbcohesion           2.424089 0.412526140\nnbpctpoverty         2.675049 0.373824975\nnbpopdensity1mi      4.822823 0.207347436\nnbviolence           3.570020 0.280110446\nnbk3favorfoodstore   4.517589 0.221357033\nvitamind3epimer      1.037610 0.963753300\nrwt                  1.210239 0.826283110\ndbp                  1.788176 0.559229062\ncv                   1.261053 0.792988047\nqrs                  1.261875 0.792471777\nsbp                  1.945465 0.514015914\nqt                   1.212909 0.824464181\nlvmecho             56.301555 0.017761499\nlvmindex            50.257911 0.019897365\nalbuminuspot         1.104439 0.905436835\nfvcpp                2.201059 0.454326743\nfev1                 4.838928 0.206657329\nhoma_b               2.979653 0.335609586\nhoma_ir              2.263362 0.441820566\nhba1c                3.265512 0.306230659\nfastinginsulin       3.097308 0.322861002\nfpg                  4.236465 0.236045863\n\n\nVIF’s are found by regressing each predictor variable on all the other predictor variables. The higher the calculated R2 value, the better the performance of the regression model. Variance Inflation Factor (VIF) is the reciprocal of 1-R2. The typical cutoff for VIF is 5. A VIF score above 5 signifies severe multicollinearity and an increase in variance of the coefficients as a result. This inflation can be a large contributing source of Type II (False Negative) error in linear regression. A VIF < 3 signifies a low correlation. Note the extreme VIF scores for several variables shown here, confirming our suspicions of multicollinearity in the data. The tolerance is the reciprocal of the VIF. It is calculated as 1-R2. Acceptable Tolerance scores are 0.25 and higher.\nTaken together, there is evidence of multicollnearity in the data that could pose a problem with interpretations of linear models. LASSO, Ridge and ElasticNet are most valuable for predictions that are more resistant to the multicollinearity problems that arise with general linear models. Next we will highlight LASSO’s built-in feature selection capabilities and demonstrate Ridge Regression’s ability to outperform LASSO given the right circumstances. The regularized model is more stable with predictions than coefficient and p-values.\nRegularization regression models are best at predictions. so, before we run regularization steps, we need to partition the data for training and testing, and find the optimal lambda in order to train the predictive models. To simplify, we will work with continuous data and variables found in several “hot spots” from the large correlation matrix above.\n\n\n\n\n\n5.3.2 LASSO Regression (L1)\n\n\n5.3.3 Finding Optimal Lambda with 10-fold Cross Validation\n\n\n    alpha     lambda\n159     1 0.01466671\n\n\n[1] 0.01466671\n\n\n\n\n\nLASSO Predicts Variable Importance using the sum of decrease in error by each variable. Relative importance is variable importance standardized to the highest valued variable.\n\n\n\n\n\nNote that LASSO has dropped 3 variables from the model: height, weight and nbcohesion. It kept variables such as bmi (a composite of height and weight). Eliminating redundant variables demonstrates LASSO’s automatic feature selection properties. These variables had very large VIF and low Tolerance scores, so dropping them from the model will likely not affect the model’s ability to predict ejection fraction.\n\n\n29 x 1 sparse Matrix of class \"dgCMatrix\"\n                            s1\n(Intercept)        62.06311823\nheight              .         \nage                 0.37433590\nweight              .         \nbmi                -0.20621469\nneck                0.55728216\nwaist               0.27193781\nnbcohesion          .         \nnbpctpoverty       -0.41411821\nnbpopdensity1mi     1.48446824\nnbviolence          0.32721465\nnbk3favorfoodstore -0.94216393\nvitamind3epimer    -0.59175868\nrwt                 0.57516426\ndbp                -0.47172460\ncv                 -0.54967558\nqrs                -0.49977220\nsbp                 0.37892530\nqt                  0.37706281\nlvmecho            -1.58162530\nlvmindex            0.83933373\nalbuminuspot       -0.17443143\nfvcpp               0.44543967\nfev1               -0.36674425\nhoma_b             -0.35596479\nhoma_ir             0.05015134\nhba1c              -0.37127668\nfastinginsulin      0.26863477\nfpg                -0.08627352\n\n\n\n\n5.3.4 Ridge Regression (L2)\n\n\n\n\n\nNote that Ridge has preserved all the variables in the model.\n\n\n29 x 1 sparse Matrix of class \"dgCMatrix\"\n                             s1\n(Intercept)        62.063118229\nheight             -0.328942291\nage                 0.367253633\nweight             -0.177554773\nbmi                -0.046283161\nneck                0.509833915\nwaist               0.284977446\nnbcohesion          0.005698985\nnbpctpoverty       -0.391973095\nnbpopdensity1mi     1.165485436\nnbviolence          0.358429644\nnbk3favorfoodstore -0.653056339\nvitamind3epimer    -0.578156859\nrwt                 0.546975215\ndbp                -0.456671077\ncv                 -0.548001203\nqrs                -0.487851438\nsbp                 0.362746921\nqt                  0.365676114\nlvmecho            -0.816777075\nlvmindex            0.156455130\nalbuminuspot       -0.187184851\nfvcpp               0.443247304\nfev1               -0.373001677\nhoma_b             -0.359786953\nhoma_ir             0.073694767\nhba1c              -0.353360710\nfastinginsulin      0.270718173\nfpg                -0.115286297\n\n\nNote that the Elastic Net has dropped only 2 of the same variables as LASSO above. The distinction between the two might become more pronounced with larger models.\n\n\n29 x 1 sparse Matrix of class \"dgCMatrix\"\n                            s1\n(Intercept)        62.06311823\nheight              .         \nage                 0.36806785\nweight             -0.01070930\nbmi                -0.20818113\nneck                0.56078455\nwaist               0.28205005\nnbcohesion          .         \nnbpctpoverty       -0.42004989\nnbpopdensity1mi     1.48073175\nnbviolence          0.33299455\nnbk3favorfoodstore -0.93908016\nvitamind3epimer    -0.59304457\nrwt                 0.57582898\ndbp                -0.47455288\ncv                 -0.55061788\nqrs                -0.50091946\nsbp                 0.38179182\nqt                  0.37820512\nlvmecho            -1.56578638\nlvmindex            0.82483823\nalbuminuspot       -0.17629896\nfvcpp               0.45225827\nfev1               -0.37729951\nhoma_b             -0.36493807\nhoma_ir             0.05402918\nhba1c              -0.37205524\nfastinginsulin      0.27569625\nfpg                -0.09518685"
  },
  {
    "objectID": "LASSO_Experiment_11_20_AW.html#comparing-the-models",
    "href": "LASSO_Experiment_11_20_AW.html#comparing-the-models",
    "title": "5  Regularized Regression and the Jackson Heart Study Dataset: Multicollinearity",
    "section": "5.1 Comparing the Models",
    "text": "5.1 Comparing the Models\nModel performance was overall very similar among the four models tested with Ridge Regression coming out slightly ahead in this instance (lowest RMSE, highest R2 values).\n\n\n           RMSE     R-squared \nLASSO      6.650703 0.04752869\nRidge      6.64124  0.0502371 \nElasticNet 6.650348 0.04763028\nOLS        6.645341 0.04906372"
  },
  {
    "objectID": "LASSO_Experiment_12_5_AW.html",
    "href": "LASSO_Experiment_12_5_AW.html",
    "title": "5  Regularized Regression and the Jackson Heart Study Dataset: Multicollinearity",
    "section": "",
    "text": "When two or more variables are highly collinear, LASSO regression will select one randomly and drop the others to zero. Elastic Net adapts the properties of both LASSO and Ridge, so dropping coefficients is less frequent than for LASSO, but can still occur. Whereas, Ridge regression will keep highly collinear variables and adjust their importance with weight. Although this technique introduces more variability in the coefficients and p-values, the prediction of the dependent variable is not affected, but keeping large models can consume large amounts of computational time. memory and power. LASSO performs well when there are more observations than variables and does well with multicollinearity, simplifying the model by dropping variables including collinear variables.\nAlthough we started by observing the full JHS Visit 1 dataset of 2653 observations and 198 variables with the correlation matrix, we simplified the final model used for the Regularization Regression methods to exclude non-numeric data and categorical data and to include variables from several “hot spots” from the larger correlation matrix shown below. It is important to note that these regularization regression techniques can be used with other methods such as logistic regression had we decided to analyze the categorical data as well. However, many of the categorical variables were, in fact, composites of or redundant variables to the continuous variables included in this study."
  },
  {
    "objectID": "LASSO_Experiment_12_5_AW.html#correlation-matrix-heat-map-and-variance-inflation-factor-vif-scores-suggest-high-multicollinearity-for-jackson-heart-study-visit-1",
    "href": "LASSO_Experiment_12_5_AW.html#correlation-matrix-heat-map-and-variance-inflation-factor-vif-scores-suggest-high-multicollinearity-for-jackson-heart-study-visit-1",
    "title": "5  Regularized Regression and the Jackson Heart Study Dataset: Multicollinearity",
    "section": "5.1 Correlation Matrix heat map and Variance Inflation Factor (VIF) Scores suggest high multicollinearity for Jackson Heart Study Visit 1",
    "text": "5.1 Correlation Matrix heat map and Variance Inflation Factor (VIF) Scores suggest high multicollinearity for Jackson Heart Study Visit 1\n\n\n\n\n\n\n\n\n\n\n\n5.1.1 Variance Inflation Factor and Tolerance Scores suggest severe multicollinearity in Jackson Heart Study Visit 1\nTaken together, there is evidence of multicollnearity in the data that could pose a problem with interpretations of linear models. Modifying linear models with regularization through LASSO, Ridge and ElasticNet are most valuable for predictions. VIF scores over 5 & Tolerance scores under 0.2 indicate severe multicollinearity. We can use regularization to shrink the importance of the coefficients that are inflated by collinearity.\n\n\n                     VIF  Tolerance\nfasthours       1.188521 0.84138180\nage             3.568913 0.28019737\nbmi            24.901065 0.04015892\nweight         25.258306 0.03959094\nneck            2.786419 0.35888353\nfastinginsulin  1.639202 0.61005295\n\n\nAlong with the severe multicollinearity found, the data for the depression model violate important assumptions for linear regression (below). The histogram of our dependent variable has more of a gamma distribution with the QQ plot showing heavy tail, also indicating that the data are not normal. Fitted Residuals plot shows a fanning distribution with error variance not constant. Finally, the scale-location plot shows the data are clustered without an equal spread of points, suggesting unequal variances. Although this dataset and model are not ideal for making predictions about depression without first finding the appropriate transformation to meet assumptions (which is beyond the scope of this study), we can still apply regularization to demonstrate the important differences among LASSO, Ridge and Elastic Net."
  },
  {
    "objectID": "LASSO_Experiment_12_5_AW.html#analysis-of-depression-model-taken-from-matrix-hot-spots-demonstrates-reduction-of-multicollinearity-and-the-lassos-feature-selection-capabilities",
    "href": "LASSO_Experiment_12_5_AW.html#analysis-of-depression-model-taken-from-matrix-hot-spots-demonstrates-reduction-of-multicollinearity-and-the-lassos-feature-selection-capabilities",
    "title": "5  Regularized Regression and the Jackson Heart Study Dataset: Multicollinearity",
    "section": "5.2 Analysis of depression model taken from matrix “hot spots” demonstrates reduction of multicollinearity and the LASSO’s feature selection capabilities",
    "text": "5.2 Analysis of depression model taken from matrix “hot spots” demonstrates reduction of multicollinearity and the LASSO’s feature selection capabilities\nNext we highlighted LASSO’s built-in feature selection capabilities and demonstrated how regularization can mediate multicollinearity. Most importantly, the regularized model is more stable with predictions than coefficient and p-values. In order to make predictions, we needed to partition the data for training and testing and find the optimal lambda in order to train the predictive models. We modeled the Total Depressive Symptoms Score (depression) as the Dependent variable (y). To simplify, we worked with continuous data and variables found in several “hot spots” from the correlation matrix above.\n\n5.2.1 Jackson Heart: LASSO (𝞪=1), L1 Norm (lm)\n\n\n5.2.2 𝞴 x |Slope|\nBoth LASSO and Ridge shrink coefficients towards constraint region, LASSO’s constraint is towards the absolute value of the slope. In order to minimize the Residual Sum of Squares, we want the modified coefficient closest to that of the OLS Beta. For LASSO, this can be on the axis, essentially zeroing out a coefficient in the model. LASSO is good for feature selection by helping to winnow out the model, and it is good for getting the VIF down (as seen below).\nOn the left is the path of coefficients shrinking to zero for the linear model,on the right is the optimal lambda is found using the largest lambda with the smallest mean squared error.\n\n\n[1] 0.009762975\n\n\n\n\n\n\n\n5.2.3 Jackson Heart: LASSO (𝞪=1), L1 Norm (Gamma Penalty)\nThe path of coefficients shrinking to zero for the LASSO lm distribution is shown on the left, gamma distribution model is shown on the right.Optimal lambda is found using the largest lambda value with the smallest mean squared error. The gamma penalty gives a larger lambda than the lm model.\n\n\n\n\n\n\n\n5.2.4 Jackson Heart: LASSO (𝞪=1), L1 Norm\nMore Variables are dropped as the Lambda value is increased. Initially, the highest VIF’s were BMI, weight, fev1, fev6 . Note that at the optimal lambda (s2), LASSO has dropped 1 variable from the lm model: fastinginsulin. It kept variables such as bmi (a composite of height and weight) in the optimal lambda model, but with larger lambda values, the highest vif variables are eliminated. Eliminating redundant variables demonstrates LASSO’s automatic feature selection properties. These variables had very large VIF and low Tolerance scores, so dropping them from the model will likely not affect the model’s ability to predict depression; however, we also ran the LASSO with the gamma penalty to see how optimal lambda changes for data with a gamma distribution (bottom panel). Note that at the optimal lambda (seg98), the LASSO has dropped 3 variables from the gamma-penalty=2 model: fastinginsulin, fasthours and weight.\n\n\n10 x 4 sparse Matrix of class \"dgCMatrix\"\n                          s1            s2            s3       s4\n(Intercept)     7.2345149003  7.2459049017  6.848887e+00 8.906514\nfasthours       0.0154461464  0.0152448395  .            .       \nage             0.0294692360  0.0294523061  1.534655e-02 .       \nweight         -0.0003486204 -0.0002235076  .            .       \nneck           -0.0802404801 -0.0801305475  .            .       \nbmi             0.0176598946  0.0172365464  .            .       \nfastinginsulin  .             .             .            .       \nhoma_ir         0.0769312162  0.0767433347  3.154321e-02 .       \naldosterone    -0.0769100321 -0.0767902446 -2.957659e-02 .       \nadiponectin    -0.0001319057 -0.0001316802 -3.294485e-05 .       \n\n\n10 x 1 sparse Matrix of class \"dgCMatrix\"\n                       seg98\nintercept       7.6329993761\nfasthours       .           \nage             0.0295323517\nweight          .           \nneck           -0.0585369916\nbmi             0.0083291197\nfastinginsulin  .           \nhoma_ir         0.0620299899\naldosterone    -0.0696243295\nadiponectin    -0.0001134787\n\n\n\n\n5.2.5 Jackson Heart: Ridge (𝞪=0), L2 Norm\n\n\n5.2.6 𝞴 x Slope2\nBoth LASSO and Ridge shrink coefficients towards a constraint region, Ridge’s constraint is towards the squared slope. In order to minimize the Residual Sum of Squares, we want the modified coefficient closest to that of the OLS Beta. However, unlike LASSO, Ridge can’t get the coefficient to all the way to zero, but it can get really close and is also good for getting the VIF down.\n\n\n[1] 0.3427968\n\n\n\n\n\nNote that Ridge has preserved all the variables in the model. No Variables were dropped as the Lambda value is increased.\n\n\n6 x 4 sparse Matrix of class \"dgCMatrix\"\n                      s1           s2           s3            s4\n(Intercept)  7.683856888  7.683856888  8.907952186 10.0177820079\nfasthours    0.015702540  0.015702540  0.004486646 -0.0051594351\nage          0.034959017  0.034959017  0.017302881 -0.0015970507\nweight      -0.009863117 -0.009863117 -0.004290117 -0.0005102981\nneck        -0.089650693 -0.089650693 -0.064795298 -0.0186218701\nbmi          0.045435093  0.045435093  0.023309123  0.0081255094\n\n\n\n\n\n\n\n5.2.7 Jackson Heart: Elastic Net (𝞪=0.5) L1 & L2 Norm\nElastic Net dropped only 2 of the same variables as LASSO above. The distinction between the two might become more pronounced with larger models.\n\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept) 10.53647059\nfasthours    0.06413026\nage          0.24773363\nbmi          0.04583096\nweight       .         \nneck        -0.30749960\n\n\n\n\n5.2.8 Jackson Heart: Comparing Models\nModel performance was overall very similar among the four models tested with LASSO Regression coming out slightly ahead in this instance (lowest RMSE, highest R2 values).\nLASSO >Elastic Net>OLS>Ridge\n\n\n           RMSE     R-squared\nLASSO      6.305145 0.3074768\nRidge      6.313302 0.3056838\nElasticNet 6.30564  0.3073681\nOLS        6.305874 0.3073168"
  },
  {
    "objectID": "summary.html#jackson-heart-study",
    "href": "summary.html#jackson-heart-study",
    "title": "7  Summary",
    "section": "7.2 Jackson Heart Study",
    "text": "7.2 Jackson Heart Study\nBoth LASSO and Ridge shrink coefficients towards a constraint region, LASSO’s constraint is towards the absolute value of the slope while Ridge’s constraint is towards the squared slope. In order to minimize the Residual Sum of Squares, we want the modified coefficient closest to that of the OLS Beta. For LASSO, this can be on the axis, essentially zeroing out a coefficient in the model. This makes LASSO good for feature selection by helping to cull out the model. Here you can see increasing Lambda values shrink the number of coefficients for LASSO and Elastic Net ( hybrid of LASSO and Ridge). All models started with 28 variables. Ordinary least squares model is included for comparison (with no penalty term).\n\n\n\nRegularization Model Type\nNumber of Coefficients\n\n\n\n\nLASSO (linear model), lambda=0.002\n27\n\n\nLASSO (linear model) optimal lambda=0.004\n27\n\n\nLASSO (linear model) lambda=0.2\n20\n\n\nLASSO (linear model) lambda=2.0\n2\n\n\nLASSO (gamma penalty)\n25\n\n\nRidge ( at all lambda values)\n28\n\n\nElastic Net at optimal lambda\n26\n\n\nOrdinary Least Squares\n28\n\n\n\nThe example of Collinearity in JHS data saw few features with large coefficients, indicating LASSO was predicted to outperform the others. In fact, model performance was overall very similar among the four models tested with LASSO Regression coming out slightly ahead in this instance (lowest RMSE, highest R2 values).\n\n\n\n\nRMSE\nR-squared\n\n\n\n\nLASSO\n6.305145\n0.3074768\n\n\nRidge\n6.313302\n0.3056838\n\n\nElasticNet\n6.30564\n0.3073681\n\n\nOLS\n6.305874\n0.3073168\n\n\n\n\nIn summary, Lasso, Ridge, and Elastic Net Regression are regularization modifications of the General Linear Model. They add bias to the model to compensate for overfitting. Multiplying coefficients using a penalty term shrinks the coefficients to add bias and can provide tolerance to multicollinearity. In Lasso’s case, coefficients can go to all the way to zero, so LASSO can be used in feature selection as well. All three techniques have their limitations and can be sensitive to the ratio of the sample size to the feature size.\nFuture studies include using Group LASSO with these datasets to get a better model for multicollinearity data, especially for the features that are redundant or can be grouped. Additionally, Adaptive LASSO is another technique that runs Ridge first then LASSO to improve performance, and Least Angle Regression (LARS) LASSO and Forward Stagewise are alternative methods for choosing Lambda."
  }
]