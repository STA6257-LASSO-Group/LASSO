[
  {
    "objectID": "dataset_methods.html",
    "href": "dataset_methods.html",
    "title": "2  Methods and Materials",
    "section": "",
    "text": "Previous studies indicate LASSO regression outperforms Ridge regression with sparse data sets in which the variable size is larger than the sample size. Conversely, Ridge regression has been shown to outperform LASSO regression with dense data in which the variable size is smaller than the sample size. The hybrid of the two, Elastic Net regression, will adapt to either type of data by using the properties of LASSO and Ridge regression together. We aim to use two data sets with different characteristics to highlight the advantages of each of the three linear regularization techniques while also using these statistical methods to model the data and make predictions.\n\n\nFirstly, we want to use a data set that has more features than observations to highlight the LASSO regression’s performance. For this situation, we will use the NCI 60 microarray data from Ashburner et al. This data set consists of 6830 genes with cancer types from 64 cancer cell lines. The format of the data set is a list with two elements: data and labs. The data element is a 64 by 6830 matrix of the data values while the labs element is a vector of the cancer cell lines listing cancer types.\n\n\n\nThe Jackson Heart Study (JHS) data has 198 variables and an n of 2653 for Visit 1 (out of 3). The JHS data will be used to highlight Ridge regression’s performance over LASSO’s when the sample size greatly outnumbers the variable size. Additionally, we are interested in using this data set to test all three methods on shrinking the multicollinearity and to test LASSO’s feature selection capabilities."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Regression is the predicting or learning of numerical features in statistics and machine learning. This process includes the prediction of numeric as well as categorical attributes. Performing a regression allows for the determination of which factors in situations matter, what is their impact, and how they influence other factors. Regression allows for data driven decisions that eliminate guesswork. ((Joshi and Saxena 2020))\nSimple linear regression finds the relationship between a dependent and independent variable whereas multiple linear regression can find the relationship between the dependent and multiple independent variables. ((Joshi and Saxena 2020))\nThe data is modeled on a graph and a line is fitted to the data. This allows for the determination of the coefficients of the independent variable or variables. ((Joshi and Saxena 2020))\n\n\nRegression line fitting is a delicate balance as the line should predict the desired value well based on several features but be applicable enough to be used repeatedly with different samples of data. The line should not be underfitted, which would not provide an accurate prediction. The line also should not be overfitted to the sample, which would cause the model not to work well with different samples and the data at large. ((Jabbar and Khan 2014))\nSeveral methods can be used to ensure appropriate line fitting. We can compare the data to the AUC Curves which allows for determining the curves predictive ability by quantifying it. The higher the AUC the more predictive the model. The data is separated into two parts. 80% of the data is labeled as the training or estimation data and 20% of the data is labeled as the validation set. Both parts of the data are tested to ensure the AUC values are similar.((Aheto et al. 2021))\nThe Penalty Method is important to prevent overfitting. We set \\(E_{train}\\) and \\(E_{test}\\) to be the test error, respectively. We then try to find a the minimal penalty such that \\(E_{test} = E_{train} + Penalty\\) ((Jabbar and Khan 2014))\nEarly Stopping Method is used for the prevention of both over and underfitting. The sample data is broken down into three parts: training, validation, and testing. This data is then broken down in to the similar 80%/20% split to estimate the line and then to validate the process. ((Jabbar and Khan 2014))\n\n\n\nBias measures the amount of deviation from the expected value is from the estimator in the models. Variance measures how far the data fluctuates. (Mehta, Bukov, Wang)\nIf we consider the bulls eye as an example: low bias indicates that the estimators are in th vicinity of the bulls eye and low variance indicates that the estimators are close together like in a cluster. ((Gudivada 2017))\n\n\n\nFigure 1"
  },
  {
    "objectID": "intro.html#regularization",
    "href": "intro.html#regularization",
    "title": "1  Introduction",
    "section": "1.2 Regularization",
    "text": "1.2 Regularization\nRegularization was introduced in the context of dealing with the matrix inverse. This inverse issue caused the answer to live outside the of the needed mathematical space. The introduction of the regularization parameter is a smoothness penalty which allowed the problem to be solved. ((Fan et al. 2006))\nThe fitting of models with large numbers of parameters also has similar issues in which makes the models unstable. This issue requires the need for regularization to get a sensible model. The LASSO regression also has a penalty to ensure the model is stable and thus works with the data. ((Fan et al. 2006))\nOrdinary Least Squares needed improvements due to the issue with large variance even though the process has a low bias. Shrinking or setting the coefficients to zero can improve the accuracy of the predictions. This process can introduce some bias but will reduce the variance which can improve the prediction accuracy. ((Hastie and Tibshirani 2015))\n\n1.2.1 LASSO\nLASSO or Least Absolute Shrinkage and Selection Operator is a regularized regression modeling method that performs variable selection and regularization. These two components allow for better accuracy in prediction and interpretation in the model. ((Emmert-Streib and Dehmer 2019)).\nIn the variable selection side, LASSO regression identifies the proper variables that will minimize prediction error and lessen the computer-intensive nature. Less variables, less computational power is needed. ((Ranstam and Cook 2018))\nThe selection of variables comes from a constraint on the model parameters. This is done by forcing the sum of the absolute value of these coefficients to be less than a fixed value \\(\\lambda\\). This constraint lowers the complexity of the model by eliminating variables from the model that are reduced to zero after the shrinkage. ((Ranstam and Cook 2018))\nThis shrinking of some of the coefficients to zero provides for an automated way for doing model selection in linear regression. ((Hastie and Tibshirani 2015))\n\\(\\lambda\\) is chosen using an automated k-fold cross-validation approach. This approach is were the data is partitioned into sub-samples of equal size. k-1 out of k sub-samples are used for developing the model with the \\(k^{th}\\) sample used to validate the model. This process is done k times to ensure each sub-set k is used to validate the model as some point. This process provides a range of values for \\(\\lambda\\). This provides a data set that allows for choosing a preferred \\(\\lambda\\). ((Ranstam and Cook 2018))\nLASSO is not perfect. LASSO trades off the potential bias of individual parameter estimates for a better overall prediction. An important disadvantage to note is that the individual regression coefficients may be less reliable to interpret individually. The focus of the LASSO regression is to provide the best combined prediction. ((Ranstam and Cook 2018))\n\n\n1.2.2 Ridge Regression\nRidge Regression is a regularized regression model that predates the LASSO. ((Hastie and Tibshirani 2015)) This method also has a shrinking effect on the coefficients of the linear model but does not reduce these coefficients to zero. ((Emmert-Streib and Dehmer 2019)). This method can simplify the problem from the original set of coefficients it does not have the added property of automated model selection."
  },
  {
    "objectID": "intro.html#feature-selection",
    "href": "intro.html#feature-selection",
    "title": "1  Introduction",
    "section": "1.3 Feature selection",
    "text": "1.3 Feature selection\nLASSO, Ridge, and Elastic Net Regression simplify the features by reducing and/or simplifying the values to zero. Due to this type of feature selection and/or reduction, the model may result in poor fitting. ((Aheto et al. 2021)) This ill-fitting is mentioned in the previous section on regression line fitting.\nModeling a situation where the number of features is larger than the number of observations is a difficult and unsolved problem in many situations. ((Freijeiro-González, Febrero-Bande, and González-Manteiga 2020))\n\n\n\n\nAheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua. 2021. “A Predictive Model, and Predictors of Under-Five Child Malaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net Regression Approaches Compare?” Preventive Medicine Reports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\nEmmert-Streib, Frank, and Matthias Dehmer. 2019. “High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection.” Machine Learning and Knowledge Extraction 27 (2): 359–83. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nFan, Jianqing, Peter Bickel, Bo Li, Alexandre Tsybakov, Sara van de Geer, B. Yu, Teofilo Valdes, Carlos Rivero, and Aad Vaart. 2006. “Regularization in Statistics.” TEST: An Official Journal of the Spanish Society of Statistics and Operations Research 15 (February): 271–344. https://doi.org/10.1007/BF02607055.\n\n\nFreijeiro-González, Laura, Manuel Febrero-Bande, and Wenceslao González-Manteiga. 2020. “A critical review of LASSO and its derivatives for variable selection under dependence among covariates.” arXiv e-Prints, December, arXiv:2012.11470. https://arxiv.org/abs/2012.11470.\n\n\nGudivada, Amy & Ding, Venkat & Apon. 2017. Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations. International Journal on Advances in Software.\n\n\nHastie, Wainwright, Trevor, and Robert Tibshirani. 2015. Statistical Learning with Sparsity: The LASSO and Generalizations. CRC Press, Taylor & Francis Group.\n\n\nJabbar, Haider Khalaf, and Rafiqul Zaman Khan. 2014. “Methods to Avoid over-Fitting and Under-Fitting in Supervised Machine Learning (Comparative Study).” In.\n\n\nJoshi, Jigyasu, and Shweta Saxena. 2020. “Regression Analysis in Data Science.” Journal of Analysis and Computation, May, 97–111.\n\n\nRanstam, J, and J A Cook. 2018. “LASSO regression.” British Journal of Surgery 105 (10): 1348–48. https://doi.org/10.1002/bjs.10895."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua.\n2021. “A Predictive Model, and Predictors of Under-Five Child\nMalaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net\nRegression Approaches Compare?” Preventive Medicine\nReports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\nEmmert-Streib, Frank, and Matthias Dehmer. 2019. “High-Dimensional\nLASSO-Based Computational Regression Models: Regularization, Shrinkage,\nand Selection.” Machine Learning and Knowledge\nExtraction 27 (2): 359–83. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nFan, Jianqing, Peter Bickel, Bo Li, Alexandre Tsybakov, Sara van de\nGeer, B. Yu, Teofilo Valdes, Carlos Rivero, and Aad Vaart. 2006.\n“Regularization in Statistics.” TEST: An Official\nJournal of the Spanish Society of Statistics and Operations\nResearch 15 (February): 271–344. https://doi.org/10.1007/BF02607055.\n\n\nFreijeiro-González, Laura, Manuel Febrero-Bande, and Wenceslao\nGonzález-Manteiga. 2020. “A critical review\nof LASSO and its derivatives for variable selection under dependence\namong covariates.” arXiv e-Prints, December,\narXiv:2012.11470. https://arxiv.org/abs/2012.11470.\n\n\nGudivada, Amy & Ding, Venkat & Apon. 2017. Data Quality\nConsiderations for Big Data and Machine Learning: Going Beyond Data\nCleaning and Transformations. International Journal on Advances in\nSoftware.\n\n\nHastie, Wainwright, Trevor, and Robert Tibshirani. 2015. Statistical\nLearning with Sparsity: The LASSO and Generalizations. CRC Press,\nTaylor & Francis Group.\n\n\nJabbar, Haider Khalaf, and Rafiqul Zaman Khan. 2014. “Methods to\nAvoid over-Fitting and Under-Fitting in Supervised Machine Learning\n(Comparative Study).” In.\n\n\nJoshi, Jigyasu, and Shweta Saxena. 2020. “Regression Analysis in\nData Science.” Journal of Analysis and Computation, May,\n97–111.\n\n\nRanstam, J, and J A Cook. 2018. “LASSO\nregression.” British Journal of Surgery 105 (10):\n1348–48. https://doi.org/10.1002/bjs.10895."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Overview of LASSO, Ridge, and Elastic Net Regularized Regression Methods",
    "section": "",
    "text": "This project will present an overview of three regularized regression methods: LASSO regression, Ridge Regression, and the Elastic Net."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "NCI60.html",
    "href": "NCI60.html",
    "title": "4  High Dimensional Data Example",
    "section": "",
    "text": "In this dataset, the data is already standardized.\n\n\n\n\n\nThe graph above shows the distribution of the gene expression level being examined.\n\n4.0.1 Calculating Model Using LASSO\nIn computing a LASSO solution to a problem, a main point of concern is the calculation of the Lambda term. In the overview section above, the lambda term was discussed as being the penalty term for the regularization. There are several R packages used to calculate Lambda in a LASSO problem. In this high dimensional example, the glmnet package will be used to estimate the optimal lambda value for the data and to generate the final model. The point of this example is to demonstrate the LASSO effect of reducing the dimensionality of a high dimensional example.\nThe cv.glmnet function uses cross-fold validation to generate LASSO regression models for the gene dataset. It outputs a series of models, each with its own lambda value. The model with the minimal lambda value is the one used for the final fitted model.\n\nset.seed(250)\n#Create model using cross fold validation and glmnet. \nmodel <- cv.glmnet(predictors, resp, alpha=1)\nbestLambda <- model$lambda.min\nplot(model)\n\n\n\n\nThe plot above shows the change in MSE for the different models compared to their lambda values. The optimal lambda value calculated was 0.1263351\nNow that the optimal lambda has been chosen, the final model will be fit to the data and the output coefficients from the 6829 original coefficients will be displayed.\n\nfinalModel <- glmnet(predictors,resp, alpha=1, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Gene Number\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] > 0)\n {rows <- nrow(coefList)\n   newRow <- c(x,coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nprint(coefList)\n\n  Gene Number Coefficient\n1        3936 0.007314000\n2        3995 0.045146244\n3        4818 0.066865827\n4        5435 0.005142151\n5        6262 0.008561928\n6        6576 0.019084629\n7        6609 0.114305087\n\neval_results(resp,finalModelPredict,geneTable)\n\n       RMSE   Rsquare\n1 0.3271616 0.4417445\n\n\nIn the final model, the 6829 original gene expression levels have been reduced down to 6 different genes that the LASSO algorithm identified as having a large impact on gene 1’s expression level."
  },
  {
    "objectID": "dataset_methods.html#references",
    "href": "dataset_methods.html#references",
    "title": "2  Methods and Materials",
    "section": "2.2 References",
    "text": "2.2 References\nAshburner M, Ball CA, Blake JA, Botstein D, Butler H, Cherry JM, Davis AP, Dolinski K, Dwight SS, Eppig JT, Harris MA, Hill DP, Issel-Tarver L, Kasarskis A, Lewis S, Matese JC, Richardson JE, Ringwald M, Rubin GM, Sherlock G. Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet. 2000 May;25(1):25-9. doi: 10.1038/75556. PMID: 10802651; PMCID: PMC3037419.\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, https://www.statlearning.com, Springer-Verlag, New York"
  },
  {
    "objectID": "JacksonHeart.html",
    "href": "JacksonHeart.html",
    "title": "3  Regularized Regression Implementation to the Jackson Heart Study Data",
    "section": "",
    "text": "The Jackson Heart Study data will be examined using to show the process difference between LASSO, Ridge Regression, Elastic Net, and a traditional OLS regression model. The value of total cholesterol will be modeled against the subject’s age, bmi, hba1c, systolic blood pressure, diastolic blood pressure, plasma glucose level, and plasma insulin level, waist circumference, and past 12 month average alcohol use. Missing observations will be corrected using the MICE algorithm.\n\n#Import data sets\nanalysis1 <- read_dta(\"analysis1.dta\")\n#View(analysis1)\ndf <- (analysis1)\n#df <- as.matrix(analysis1)\ndf<- df %>% select(c(\"totchol\",\"age\",\"bmi\",\"sbp\",\"dbp\",\"hba1c\",\"fpg\",\"fastinginsulin\",\"alcw\", \"waist\"))\nmcar <- na.test(df)\n\n Little's MCAR Test\n\n     n nIncomp nPattern   chi2  df  pval \n  2653     315       17 336.83 123 0.000\n\n\nThe results of the MCAR test are significant, so the data is not missing at random.\n\ncleanedData <- mice(df, printFlag = FALSE)\ndf <- complete(cleanedData,1)\ndfpred <- df %>% select(c(-\"totchol\"))\ndfResp <- df %>% select(c(\"totchol\"))\ndfResp <- data.matrix(dfResp)\ndfpred <- data.matrix(dfpred)\n\nThe data is now cleaned and complete. It will be passed through several regression models, beginning with a traditional OLS regression.\n\nggplot(df)+aes(totchol)+geom_histogram(binwidth=30)+labs(x=\"Total Cholesterol\")\n\n\n\nmodel = lm(totchol ~ age+bmi+sbp+dbp+hba1c+fpg+fastinginsulin+alcw+waist, data=df)\nsummary(model)\n\n\nCall:\nlm(formula = totchol ~ age + bmi + sbp + dbp + hba1c + fpg + \n    fastinginsulin + alcw + waist, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-102.527  -26.410   -2.847   23.672  136.583 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    145.986930   9.204247  15.861  < 2e-16 ***\nage              0.467947   0.075269   6.217 5.87e-10 ***\nbmi             -0.023571   0.190842  -0.124   0.9017    \nsbp              0.065757   0.064617   1.018   0.3089    \ndbp              0.244680   0.111664   2.191   0.0285 *  \nhba1c           -0.054045   1.105112  -0.049   0.9610    \nfpg              0.047838   0.042414   1.128   0.2595    \nfastinginsulin   0.098154   0.073021   1.344   0.1790    \nalcw             0.008055   0.150608   0.053   0.9574    \nwaist           -0.045931   0.083404  -0.551   0.5819    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.15 on 2643 degrees of freedom\nMultiple R-squared:  0.02996,   Adjusted R-squared:  0.02665 \nF-statistic: 9.069 on 9 and 2643 DF,  p-value: 1.291e-13\n\ncorrplot::corrplot(cor(df), method=\"number\")\n\n\n\nols_vif_tol(model)\n\n       Variables Tolerance      VIF\n1            age 0.7023951 1.423700\n2            bmi 0.3095886 3.230093\n3            sbp 0.5408636 1.848895\n4            dbp 0.6000756 1.666457\n5          hba1c 0.3701256 2.701785\n6            fpg 0.3818471 2.618849\n7 fastinginsulin 0.7650009 1.307188\n8           alcw 0.9502383 1.052368\n9          waist 0.3120253 3.204868\n\n\nThe correlation matrix and the VIF score show some correlation between waist and bmi, as well as systolic and diastolic blood pressures. In this approach, if we choose to remove the variables not considered significant, we would lose the information they provide to the model.\n\n3.0.1 Ridge Regression Model\n\nmodel <- cv.glmnet(dfpred, dfResp, alpha=0)\nbestLambda <- model$lambda.min\nbestLambda\n\n[1] 7.873646\n\nplot(model)\n\n\n\nfinalModel <- glmnet(dfpred,dfResp, alpha=0, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = dfpred)\nmodel$glmnet.fit %>% varImp(lambda = model$lambda.min, scale=F)\n\n                   Overall\nage            0.368774448\nbmi            0.062205251\nsbp            0.093749884\ndbp            0.165095572\nhba1c          0.271309209\nfpg            0.037346515\nfastinginsulin 0.073285642\nalcw           0.005590018\nwaist          0.019800229\n\neval_results(dfResp,finalModelPredict,df)\n\n      RMSE    Rsquare\n1 38.09026 0.02916224\n\n\nIn the Ridge regression, each variable has now had their coefficient scaled based on how much importance they add to the model. No variable can be eliminated using this method, but the penalty term can cause their coefficient, and therefore their weight, to approach 0.\n\n\n3.0.2 LASSO Regression Model\n\nmodel <- cv.glmnet(dfpred, dfResp, alpha=1)\nbestLambda <- model$lambda.min\nbestLambda\n\n[1] 0.5180334\n\nplot(model)\n\n\n\nfinalModel <- glmnet(dfpred,dfResp, alpha=1, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] > 0)\n {print(coefTable[x,1])  \n   } \n}\n\n[1] 151.342\n[1] 0.4198323\n[1] 0.06816675\n[1] 0.1637407\n[1] 0.03362986\n[1] 0.03113286\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = dfpred)\nmodel$glmnet.fit %>% varImp(lambda = model$lambda.min, scale=F)\n\n                  Overall\nage            0.41978719\nbmi            0.00000000\nsbp            0.06820686\ndbp            0.16369404\nhba1c          0.00000000\nfpg            0.03363147\nfastinginsulin 0.03112792\nalcw           0.00000000\nwaist          0.00000000\n\neval_results(dfResp,finalModelPredict,df)\n\n      RMSE    Rsquare\n1 38.09565 0.02888725\n\n\nIn the LASSO regression, the penalty term can cause the variables to be dropped from the model by setting their coefficient to 0. This occurs when their importance is very low.\n\n\n3.0.3 Elastic Net Model\n\nmodel <- cv.glmnet(dfpred, dfResp, alpha=.5)\nbestLambda <- model$lambda.min\nbestLambda\n\n[1] 0.7837465\n\nplot(model)\n\n\n\nfinalModel <- glmnet(dfpred,dfResp, alpha=.5, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] > 0)\n {print(coefTable[x,1])  \n   } \n}\n\n[1] 149.9628\n[1] 0.4259593\n[1] 0.06931315\n[1] 0.1781041\n[1] 0.03650472\n[1] 0.04354466\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = dfpred)\n\neval_results(dfResp,finalModelPredict,df)\n\n     RMSE    Rsquare\n1 38.0891 0.02922102\n\nmodel$glmnet.fit %>% varImp(lambda = model$lambda.min, scale=F)\n\n                   Overall\nage            0.425099286\nbmi            0.008286747\nsbp            0.069654076\ndbp            0.177422447\nhba1c          0.000000000\nfpg            0.036400286\nfastinginsulin 0.043418968\nalcw           0.000000000\nwaist          0.003963468\n\n\nSimilarly to the LASSO regression, the Elastic Net method can also cause variables to be dropped from the model when they offer no importance to the prediction. The value of alpha in the glmnet package determines the strength of the lambda parameter."
  }
]