[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LASSO",
    "section": "",
    "text": "This is a Quarto book. Testing a change.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Regression is the predicting or learning of numerical features in statistics and machine learning. This includes the prediction of numeric as well as categorical attributes. The process of performing a regression allows for the determination of which factors in situations matter, what is their impact, and how they influence other factors. Regression allows for data driven decisions that eliminate guesswork. (Joshi and Saxena (2020))\nSimple linear regression finds the relationship between a dependent and independent variable whereas multiple linear regression can find the relationship between the dependent and multiple dependent variables. (Joshi and Saxena (2020))\nThe data is modeled on a graph and a line is fitted to the data. This allows for the determination of the coefficients of the independent variable or variables. (Joshi and Saxena (2020))\n\n\nRegression line fitting is a delicate balance as the line should predict the desired value well based on several features but be applicable enough to be used repeatedly with different samples of data. The line should not be underfitted, which would not provide an accurate prediction, but also should not be overfitted to the sample, the model doesn’t stand up well to different samples and the data at large. (Jabbar and Khan (2014))\nPrior to testing, the sample data is randomly split into two sections. 80% of the data is labeled as the training or estimation data and 20% of the data is labeled as the validation set. ((Aheto20?)) This allows for the fitting to be monitored. Various tests can determine the goodness of fit as both sets of data are tested to ensure similarity. Methods of avoiding a misfit include Penalty Methods and Early Stopping Method (Khan and Allamy)"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "intro.html#regularization",
    "href": "intro.html#regularization",
    "title": "1  Introduction",
    "section": "1.2 Regularization",
    "text": "1.2 Regularization\nRegularization was introduced in the context of dealing with the matrix inverse. This inverse issue caused the answer to live outside the of the needed mathematical space. The introduction of the regularization parameter is a smoothness penalty which allowed the problem to be solved. (Bickel and Li)\nThe fitting of models with large numbers of parameters also has similar issues in which makes the models unstable. This issue requires the need for regularization to get a sensible model. The LASSO regression also has a penalty to ensure the model is stable and thus works with the data (Bickel and Li)\nOrdinary Least Squares needed improvements due to the issue with large variance even though the process has a low bias. Shrinking or setting the coefficients to zero can improve the accuracy of the predictions. This process can introduce some bias but will reduce the variance which can improve the prediction accuracy. (Hastie, Taibshirani, Wainwright)\n\n1.2.1 LASSO\nLASSO or Least Absolute Shrinkage and Selection Operator is a regularized regression modeling method that performs variable selection and regularization. These two components allow for better accuracy in prediction and interpretability in the model. (Emmert-Streib and Dehmer (2019)). The LASSO method combines the L1 constraint, which bounds the sum of the coefficients of the absolute values and the least-squares loss. This is a method that allows for the shrinking of the coefficients in a linear model setting some to zero. (Hastie, Taibshirani, Wainwright)\nSince LASSO can shrink some of the coefficients to zero this method provides for an automated way for doing model selection in linear regression. (Hastie, Taibshirani, Wainwright)\n\n\n1.2.2 Ridge Regression\nRidge Regression is a regularized regression model that predates the LASSO. (Hastie, Taibshirani, Wainwright) This method also has a shrinking effect on the coefficients of the linear model but does not reduce these coefficients to zero. (Emmert-Streib and Dehmer (2019)). This method can simplify the problem from the original set of coefficients it does not have the added property of automated model selection."
  },
  {
    "objectID": "intro.html#feature-selection",
    "href": "intro.html#feature-selection",
    "title": "1  Introduction",
    "section": "1.3 Feature selection",
    "text": "1.3 Feature selection\nLASSO, Ridge, and Elastic Net Regression simplify the features by reducing and/or simplifying the values to zero. Due to this type of feature selection and/or reduction, the model may result in poor fitting. (Aheto et al. (2021)) This ill-fitting is mentioned in the previous section on regression line fitting.\nModeling a situation where the number of features is larger than the number of observations is a difficult and unsolved problem in many situations. (Freijeiro-González, Febrero-Bande, and González-Manteiga (2020))\n\n\n\n\nAheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua. 2021. “A Predictive Model, and Predictors of Under-Five Child Malaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net Regression Approaches Compare?” Preventive Medicine Reports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\nEmmert-Streib, Frank, and Matthias Dehmer. 2019. “High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection.” Machine Learning and Knowledge Extraction 27 (2): 359–83. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nFreijeiro-González, Laura, Manuel Febrero-Bande, and Wenceslao González-Manteiga. 2020. “A critical review of LASSO and its derivatives for variable selection under dependence among covariates.” arXiv e-Prints, December, arXiv:2012.11470. https://arxiv.org/abs/2012.11470.\n\n\nJabbar, Haider Khalaf, and Rafiqul Zaman Khan. 2014. “Methods to Avoid over-Fitting and Under-Fitting in Supervised Machine Learning (Comparative Study).” In.\n\n\nJoshi, Jigyasu, and Shweta Saxena. 2020. “Regression Analysis in Data Science.” Journal of Analysis and Computation, May, 97–111."
  }
]