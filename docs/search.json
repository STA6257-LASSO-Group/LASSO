[
  {

    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Regression is the predicting or learning of numerical features in statistics and machine learning. This includes the prediction of numeric as well as categorical attributes. The process of performing a regression allows for the determination of which factors in situations matter, what is their impact, and how they influence other factors. Regression allows for data driven decisions that eliminate guesswork. ((Joshi and Saxena 2020))\nSimple linear regression finds the relationship between a dependent and independent variable whereas multiple linear regression can find the relationship between the dependent and multiple dependent variables. ((Joshi and Saxena 2020))\nThe data is modeled on a graph and a line is fitted to the data. This allows for the determination of the coefficients of the independent variable or variables. ((Joshi and Saxena 2020))\n\n\nRegression line fitting is a delicate balance as the line should predict the desired value well based on several features but be applicable enough to be used repeatedly with different samples of data. The line should not be underfitted, which would not provide an accurate prediction, but also should not be overfitted to the sample, the model doesn’t stand up well to different samples and the data at large. ((Jabbar and Khan 2014))\nPrior to testing, the sample data is randomly split into two sections. 80% of the data is labeled as the training or estimation data and 20% of the data is labeled as the validation set. ((Aheto et al. 2021)) This allows for the fitting to be monitored. Various tests can determine the goodness of fit as both sets of data are tested to ensure similarity. Methods of avoiding a misfit include Penalty Methods and Early Stopping Method (Khan and Allamy)"
  },
  {
    "objectID": "intro.html#regularization",
    "href": "intro.html#regularization",
    "title": "1  Introduction",
    "section": "1.2 Regularization",
    "text": "1.2 Regularization\nRegularization was introduced in the context of dealing with the matrix inverse. This inverse issue caused the answer to live outside the of the needed mathematical space. The introduction of the regularization parameter is a smoothness penalty which allowed the problem to be solved. (Bickel and Li)\nThe fitting of models with large numbers of parameters also has similar issues in which makes the models unstable. This issue requires the need for regularization to get a sensible model. The LASSO regression also has a penalty to ensure the model is stable and thus works with the data (Bickel and Li)\nOrdinary Least Squares needed improvements due to the issue with large variance even though the process has a low bias. Shrinking or setting the coefficients to zero can improve the accuracy of the predictions. This process can introduce some bias but will reduce the variance which can improve the prediction accuracy. ([James et al. (2021)])\n\n1.2.1 LASSO\nLASSO or Least Absolute Shrinkage and Selection Operator is a regularized regression modeling method that performs variable selection and regularization. These two components allow for better accuracy in prediction and interpretability in the model. ((Emmert-Streib and Dehmer 2019)). The LASSO method combines the L1 constraint, which bounds the sum of the coefficients of the absolute values and the least-squares loss. This is a method that allows for the shrinking of the coefficients in a linear model setting some to zero. ([James et al. (2021)])\nSince LASSO can shrink some of the coefficients to zero this method provides for an automated way for doing model selection in linear regression. ([James et al. (2021)])\n\n\n1.2.2 Ridge Regression\nRidge Regression is a regularized regression model that predates the LASSO. ([James et al. (2021)]) This method also has a shrinking effect on the coefficients of the linear model but does not reduce these coefficients to zero. ((Emmert-Streib and Dehmer 2019)). This method can simplify the problem from the original set of coefficients it does not have the added property of automated model selection."
  },
  {
    "objectID": "intro.html#feature-selection",
    "href": "intro.html#feature-selection",
    "title": "1  Introduction",
    "section": "1.3 Feature selection",
    "text": "1.3 Feature selection\nLASSO, Ridge, and Elastic Net Regression simplify the features by reducing and/or simplifying the values to zero. Due to this type of feature selection and/or reduction, the model may result in poor fitting. ((Aheto et al. 2021)) This ill-fitting is mentioned in the previous section on regression line fitting.\nModeling a situation where the number of features is larger than the number of observations is a difficult and unsolved problem in many situations. ((Freijeiro-González, Febrero-Bande, and González-Manteiga 2020))\n\n\n\n\nAheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua. 2021. “A Predictive Model, and Predictors of Under-Five Child Malaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net Regression Approaches Compare?” Preventive Medicine Reports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\nEmmert-Streib, Frank, and Matthias Dehmer. 2019. “High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection.” Machine Learning and Knowledge Extraction 27 (2): 359–83. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nFreijeiro-González, Laura, Manuel Febrero-Bande, and Wenceslao González-Manteiga. 2020. “A critical review of LASSO and its derivatives for variable selection under dependence among covariates.” arXiv e-Prints, December, arXiv:2012.11470. https://arxiv.org/abs/2012.11470.\n\n\nJabbar, Haider Khalaf, and Rafiqul Zaman Khan. 2014. “Methods to Avoid over-Fitting and Under-Fitting in Supervised Machine Learning (Comparative Study).” In.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning, with Applications in r. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nJoshi, Jigyasu, and Shweta Saxena. 2020. “Regression Analysis in Data Science.” Journal of Analysis and Computation, May, 97–111."
  },
  {


    "text": "Previous studies indicate LASSO regression outperforms Ridge regression with sparse data sets in which the variable size is larger than the sample size. Conversely, Ridge regression has been shown to outperform LASSO regression with dense data in which the variable size is smaller than the sample size. The hybrid of the two, Elastic Net regression, will adapt to either type of data by using the properties of LASSO and Ridge regression together. We aim to use two data sets with different characteristics to highlight the advantages of each of the three linear regularization techniques while also using these statistical methods to model the data and make predictions.\n\n\nFirstly, we want to use a data set that has more features than observations to highlight the LASSO regression’s performance. For this situation, we will use the NCI 60 microarray data from Ashburner et al. This data set consists of 6830 genes with cancer types from 64 cancer cell lines. The format of the data set is a list with two elements: data and labs. The data element is a 64 by 6830 matrix of the data values while the labs element is a vector of the cancer cell lines listing cancer types.\n\n\n\nThe Jackson Heart Study (JHS) data has 198 variables and an n of 2653 for Visit 1 (out of 3). The JHS data will be used to highlight Ridge regression’s performance over LASSO’s when the sample size greatly outnumbers the variable size. Additionally, we are interested in using this data set to test all three methods on shrinking the multicollinearity and to test LASSO’s feature selection capabilities.\n \n\n\n\n\nAshburner M, Ball CA, Blake JA, Botstein D, Butler H, Cherry JM, Davis AP, Dolinski K, Dwight SS, Eppig JT, Harris MA, Hill DP, Issel-Tarver L, Kasarskis A, Lewis S, Matese JC, Richardson JE, Ringwald M, Rubin GM, Sherlock G. Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet. 2000 May;25(1):25-9. doi: 10.1038/75556. PMID: 10802651; PMCID: PMC3037419.\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, https://www.statlearning.com, Springer-Verlag, New York\nHow do we cite JHS?"

    "text": "LASSO has been shown to outperform Ridge with sparse data where p>>n. Conversely, Ridge has been shown to outperformLASSO with dense data where p < n. The hybrid of the two, Elastic Net, will adapt to either type of data by using properties of LASSO and Ridge. We aim to use two data sets to highlight the advantages of each of the three regularized linear techniques while also using these statistical methods to model the data and make predictions.\nWe first want to use a data set that has more features than observations. For this situation, we will use….\nThis one looks good to me:\n\n\n\n\n\nNCI microarray data. The data contains expression levels on 6830 genes from 64 cancer cell lines. Cancer type is also recorded.\n\n\n\nNCI60\n\n\n\nThe format is a list containing two elements: data and labs.\ndata is a 64 by 6830 matrix of the expression values while labs is a vector listing the cancer types for the 64 cell lines.\n\n\n\nThe data come from Ross et al. (Nat Genet., 2000). More information can be obtained at http://genome-www.stanford.edu/nci60/\n\n\n\nJames, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, https://www.statlearning.com, Springer-Verlag, New York\n\n\n\ntable(NCI60$labs)\nThe Jackson Heart Study (JHS) data has 198 variables and an n of 2653 for Visit 1 (out of 3). The JHS data will be used to highlight Ridge regression’s performance over LASSO when n>>p. Additionally, we are interested in this data set to test all three methods on shrinking the multicollinearity and to test LASSO’s feature selection capabilities.\nAcknowledgement:\nThe Jackson Heart Study (JHS) is supported and conducted in collaboration with Jackson State University (HHSN268201800013I), Tougaloo College (HHSN268201800014I), the Mississippi State Department of Health (HHSN268201800015I) and the University of Mississippi Medical Center (HHSN268201800010I, HHSN268201800011I and HHSN268201800012I) contracts from the National Heart, Lung, and Blood Institute (NHLBI) and the National Institute on Minority Health and Health Disparities (NIMHD). The authors also wish to thank the staffs and participants of the JHS.\n \n \nDisclaimer:\nThe views expressed in this manuscript are those of the authors and do not necessarily represent the views of the National Heart, Lung, and Blood Institute; the National Institutes of Health; or the U.S. Department of Health and Human Services.\n \n \n\n\n\nEmail all inquires regarding manuscript proposals, manuscripts or abstracts to: jhspub@umc.edu"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua.\n2021. “A Predictive Model, and Predictors of Under-Five Child\nMalaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net\nRegression Approaches Compare?” Preventive Medicine\nReports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\nEmmert-Streib, Frank, and Matthias Dehmer. 2019. “High-Dimensional\nLASSO-Based Computational Regression Models: Regularization, Shrinkage,\nand Selection.” Machine Learning and Knowledge\nExtraction 27 (2): 359–83. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nFreijeiro-González, Laura, Manuel Febrero-Bande, and Wenceslao\nGonzález-Manteiga. 2020. “A critical review\nof LASSO and its derivatives for variable selection under dependence\namong covariates.” arXiv e-Prints, December,\narXiv:2012.11470. https://arxiv.org/abs/2012.11470.\n\n\nJabbar, Haider Khalaf, and Rafiqul Zaman Khan. 2014. “Methods to\nAvoid over-Fitting and Under-Fitting in Supervised Machine Learning\n(Comparative Study).” In.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning, with Applications in\nr. https://doi.org/10.1007/978-1-0716-1418-1.\n\n\nJoshi, Jigyasu, and Shweta Saxena. 2020. “Regression Analysis in\nData Science.” Journal of Analysis and Computation, May,\n97–111."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LASSO",
    "section": "",
    "text": "This is a Quarto book. Testing a change.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  }
]