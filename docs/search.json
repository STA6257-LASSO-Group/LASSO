[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Regression is the predicting or learning of numerical features in statistics and machine learning. This process includes the prediction of numeric as well as categorical attributes. Performing a regression allows for the determination of which factors in situations matter, what is their impact, and how they influence other factors. Regression allows for data driven decisions that eliminate guesswork. (Joshi and Saxena 2020)\nSimple linear regression finds the relationship between a dependent and independent variable whereas multiple linear regression can find the relationship between the dependent and multiple independent variables. (Joshi and Saxena 2020)\nThe data is modeled on a graph and a line is fitted to the data. This allows for the calculation of the coefficients of the independent variable or variables. ((Joshi and Saxena 2020))\n\n\nRegression line fitting is a delicate balance as the line should predict the desired value well based on several features but be applicable enough to be used repeatedly with different samples of data. The line should not be under-fitted, which would not provide an accurate prediction. The line also should not be over-fitted to the sample, which would cause the model not to work well with different samples and the data at large. ((Jabbar and Khan 2014))\nSeveral methods can be used to ensure appropriate line fitting. We can compare the data to the AUC Curves which determining the curves predictive ability by quantifying with the Area Under the Curve. The higher the AUC the more predictive the model. The data is separated into two parts. 80% of the data is labeled as the training or estimation data and 20% of the data is labeled as the validation set. Both parts of the data are tested to ensure the AUC values are similar.((Aheto et al. 2021))\nThe penalty method is important to prevent over-fitting. We set \\(E_{train}\\) and \\(E_{test}\\) to be the training set error and test error, respectively. We then try to find a the minimal penalty such that \\(E_{test} = E_{train} + Penalty\\) (Jabbar and Khan 2014)\nThe early stopping method is used for the prevention of both over and under-fitting. The sample data is broken down into three parts: training, validation, and testing. This data is then broken down in to the similar 80%/20% split to estimate the line and then to validate the process. (Jabbar and Khan 2014) ### Bias and variance\nBias measures the amount of deviation from the expected value is from the estimator in the models. Variance measures how far the data fluctuates. (Mehta, Bukov, Wang)\nIf we consider the bulls eye as an example: low bias indicates that the estimators are in the vicinity of the bulls eye and low variance indicates that the estimators are close together like in a cluster. (Gudivada 2017)\n\n\n\nGudivada, Venkat & Apon, Amy & Ding, Junhua. (2017). Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations. International Journal on Advances in Software. Figure 1"
  },
  {
    "objectID": "intro.html#regularization",
    "href": "intro.html#regularization",
    "title": "1  Introduction",
    "section": "1.2 Regularization",
    "text": "1.2 Regularization\nRegularization was introduced in the context of dealing with the matrix inverse. This inverse issue caused the answer to live outside of the needed mathematical space. The introduction of the regularization parameter is a smoothness penalty which allowed the problem to be solved. (Fan et al. 2006)\nThe fitting of models with large numbers of parameters also has similar issues in which makes the models unstable. This issue requires the need for regularization to get a sensible model. The LASSO regression also has a penalty to ensure the model is stable and thus works with the data. (Fan et al. 2006)\nOrdinary least squares needed improvements due to the issue with large variance even though the process has a low bias. Shrinking or setting the coefficients to zero can improve the accuracy of the predictions. This process can introduce some bias but will reduce the variance which can improve the prediction accuracy. (Hastie and Tibshirani 2015)\n\n1.2.1 LASSO\nLASSO or Least Absolute Shrinkage and Selection Operator is a regularized regression modeling method that performs variable selection and regularization. These two components allow for better accuracy in prediction and interpretation in the model. (Emmert-Streib and Dehmer 2019).\nIn the variable selection side, LASSO regression identifies the proper variables that will minimize prediction error and lessen the computer-intensive nature. Less variables mean less computational power is needed. (Ranstam and Cook 2018)\nThe selection of variables comes from a constraint on the model parameters. This is done by forcing the sum of the absolute value of these coefficients to be less than a fixed value \\(\\lambda\\). This constraint lowers the complexity of the model by eliminating variables from the model that are reduced to zero after the shrinkage. (Ranstam and Cook 2018)\nThis shrinking of some of the coefficients to zero provides for an automated way for doing model selection in linear regression. (Hastie and Tibshirani 2015)\n\\(\\lambda\\) is chosen using an automated k-fold cross-validation approach. This approach is were the data is partitioned into sub-samples of equal size. k-1 out of k sub-samples are used for developing the model with the \\(k^{th}\\) sample used to validate the model. This process is done k times to ensure each sub-set k is used to validate the model as some point. This process provides a range of values for \\(\\lambda\\) and provides a data set to determine a preferred \\(\\lambda\\). (Ranstam and Cook 2018)\nLASSO is not perfect. LASSO trades off the potential bias of individual parameter estimates for a better overall prediction. (Ranstam, Cook). This method allows for the determination of a smaller subset of predictors that shows the strongest effects on the model. ((Hastie and Tibshirani 2015)) Important disadvantage to note is that the individual regression coefficients may be less reliable to interpret individually (Ranstam and Cook 2018). Also, LASSO is not capable of selecting more predictors than the number of of observations in the sample.((Emmert-Streib and Dehmer 2019)).\nThe focus of the LASSO regression is to provide the best overall prediction and to reduce the number of variables to identify the predictors of importance not to interpret the meaning of individual predictor coefficients or keep excessive predictors. (Ranstam and Cook 2018).\n\n\n1.2.2 Ridge Regression\nRidge Regression is a regularized regression model that predates LASSO. ((Hastie and Tibshirani 2015)). Again, the need for improving the ordinary least squares comes from the fact that these models have a low bias but large variance. We can trade off some of the large variance by introducing some estimation bias allowing for accuracy of the prediction to improve. The preferred \\(\\lambda\\) in this model is different from the LASSO \\(\\lambda\\) as this ridge parameter reduces the coefficients toward zero without eliminating the the variables completely by reducing coefficients to zero.((Emmert-Streib and Dehmer 2019))\nRidge Regression works best when the ordinary least squares estimates have a high variance and the number of predictors is larger than the number of samples. The benefit of this method is the trade off of reducing the variance by increasing a low bias which can improve the prediction accuracy of the model.((Emmert-Streib and Dehmer 2019))\nThe downfall with this method is the reduction of the coefficients toward zero not to zero. Ridge regression therefore does not provide variable selection as the LASSO method. ((Emmert-Streib and Dehmer 2019))\n\n\n1.2.3 Elastic Net Regularization\nThe elastic net was designed to improve the LASSO method as well as combine the benefits of LASSO and ridge regression. The elastic net has the benefit of LASSO regression as this method can perform variable selection and the benefit of ridge regression since it can be used in cases where the number of predictors is larger than the number of observations. Recall that ridge regression cannot perform variable selection and LASSO cannot manage cases with the number of predictors is much larger than the number of observations.((Emmert-Streib and Dehmer 2019))\nAn issue that can arise from the LASSO is in situations with strongly correlated predictors the variable selection method keeps only the strongest variable. Whereas, the elastic net tends to keep these strongly correlated predictors together in the model with its underlying grouping technique. ((Emmert-Streib and Dehmer 2019))\nThe elastic net is able to blend the methods of LASSO and ridge regression by combining the squared penalty both methods and weighing them based on the number of correlated predictors. ((Hastie and Tibshirani 2015))\nIt does this by including a L2 norm penalty term in addition to the LASSO’s L1 norm penalty term. Similarly to the LASSO, it attempts to improve upon linear regression’s predictive accuracy along with the model’s interpretability in the presence of many predictors. Specifically, elastic net regularization attempts to improve upon LASSO in three scenarios:\n\nThe case where there are more predictors than observations (p > n). In this case, LASSO will predict at most n predictors.\nGrouped variables with high pairwise correlations will cause LASSO to pick only one of the variables.\nIn the case of high multicollinearity in traditional regression scenarios (n < p), LASSO’s performance is weaker than ridge regression. This means that the model does not benefit from LASSO’s feature selection properties.\n\nThe Elastic Net combines qualities from LASSO and Ridge Regression to create a method that deals well with multicollinearity while also employing LASSO-like feature selection. (Zou and Hastie 2005)"
  },
  {
    "objectID": "intro.html#why-utilize-regularized-regression-methods",
    "href": "intro.html#why-utilize-regularized-regression-methods",
    "title": "1  Introduction",
    "section": "1.3 Why Utilize Regularized Regression Methods?",
    "text": "1.3 Why Utilize Regularized Regression Methods?\nRegularized Regression Methods seek generally to improve the Ordinary Least Squares estimates in the regression problem. The two general areas that these methods attempt to improve are prediction accuracy in the model and model interpretability. Prediction accuracy concerns come from Ordinary Least Squares estimates tending to be overfit (having low bias and high variance). Several underlying issues can cause overfitting, including having too many parameters in the model, but the end result is that the model will not have high prediction accuracy.\nTo try and improve the prediction accuracy issue in these sorts of models, the regularization techniques try to alter the parameters (or their coefficients) in order to remove the high variance. This is often done at the expense of bias. The three regularization techniques handle this in different ways:\n\nRidge Regression: As stated above, this method shrinks the coefficients in the model according to a shrinkage penalty term based on the L2 norm and the sum of squared residuals (RSS). The coefficients that have the least influence in the model tend towards 0 the fastest. This reduces their impact on the model and typically lowers the variance, especially when multicollinearity is present.\nLASSO Regression: This method uses a penalty term based on the L1 norm and the RSS to shrink, and even eliminate, coefficients from the model. As coefficients shrink, their impact on the model is reduced, but when a coefficient is dropped completely, the model simplifies further, helping to improve overfitting.\n\nElastic Net Regularization: This is an extension of LASSO where an additional L2 norm penalty term is added to the LASSO. This serves to give LASSO qualities of Ridge regression and helps to improve prediction accuracy by shrinking coefficients and removing unimportant predictors.\n\nThe other main area that regularized regression methods attempt to improve is model interpretability. In this case, the LASSO and Elastic Net Regularization methods improve model interpretability by removing unimportant predictors from the model. This is also called feature selection. A downside of Ridge Regression is that it does not perform feature selection and is unable to improve model interpretability since all predictors remain in the model. In cases where there are high numbers of predictors, Ridge Regression, then, can produce a model that has good predictive ability, but poor interpretability. (Zou and Hastie 2005) (Tibshirani 1996)\n\n\n\n\nAheto, Justice Moses K., Henry Duah, Pascal Agbadi, and Emmanuel Nakua. 2021. “A Predictive Model, and Predictors of Under-Five Child Malaria Prevalence in Ghana: How Do LASSO, Ridge and Elastic Net Regression Approaches Compare?” Preventive Medicine Reports 23 (June): 101475. https://doi.org/10.1016/j.pmedr.2021.101475.\n\n\nEmmert-Streib, Frank, and Matthias Dehmer. 2019. “High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection.” Machine Learning and Knowledge Extraction 27 (2): 359–83. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nFan, Jianqing, Peter Bickel, Bo Li, Alexandre Tsybakov, Sara van de Geer, B. Yu, Teofilo Valdes, Carlos Rivero, and Aad Vaart. 2006. “Regularization in Statistics.” TEST: An Official Journal of the Spanish Society of Statistics and Operations Research 15 (February): 271–344. https://doi.org/10.1007/BF02607055.\n\n\nGudivada, Amy & Ding, Venkat & Apon. 2017. Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations. International Journal on Advances in Software.\n\n\nHastie, Wainwright, Trevor, and Robert Tibshirani. 2015. Statistical Learning with Sparsity: The LASSO and Generalizations. CRC Press, Taylor & Francis Group.\n\n\nJabbar, Haider Khalaf, and Rafiqul Zaman Khan. 2014. “Methods to Avoid over-Fitting and Under-Fitting in Supervised Machine Learning (Comparative Study).” In.\n\n\nJoshi, Jigyasu, and Shweta Saxena. 2020. “Regression Analysis in Data Science.” Journal of Analysis and Computation, May, 97–111.\n\n\nRanstam, J, and J A Cook. 2018. “LASSO regression.” British Journal of Surgery 105 (10): 1348–48. https://doi.org/10.1002/bjs.10895.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267–88. http://www.jstor.org/stable/2346178.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society. Series B (Statistical Methodology) 67 (2): 301–20. http://www.jstor.org/stable/3647580."
  },
  {
    "objectID": "HumanFreedomIndex.html",
    "href": "HumanFreedomIndex.html",
    "title": "4  High Dimensional Data Example with Human Freedom Index",
    "section": "",
    "text": "The techniques in this demonstration utilize the glmnet package to fit the regularized models. More information about this package can be found at (“An Introduction to ‘Glmnet‘” 2021)\nFor this demonstration, the political freedom homicide variable will be the response variable. Each of the other distinct variables (excluding indexes and total scores) will be used as predictors.\n\n\n\n\n4.0.1 Trimming and Fitting a Linear Model\n\n#Human Freedom Index Linear Model\nfreedomData <- hfi\nfreedomData <- freedomData %>% filter(year==2016)%>% select(c(-\"year\",-\"ISO_code\",-\"countries\",-\"region\",-\"ef_score\",-\"ef_rank\",-\"hf_rank\",-\"hf_quartile\",-\"pf_score\",-\"pf_rank\",-\"pf_religion_estop_establish\",-\"pf_religion_estop_operate\",-\"pf_identity_legal\",-\"pf_rol_procedural\",-\"pf_rol_civil\",-\"pf_rol_criminal\",-\"pf_ss_women_inheritance_widows\",-\"pf_ss_women_inheritance_daughters\",-\"pf_association_political_establish\",-\"pf_association_political_operate\",-\"pf_association_sport_operate\",-\"pf_association_sport_establish\",-\"pf_identity_divorce\",-\"pf_association_prof_operate\",-\"pf_association_prof_establish\"))%>%mutate(id = row_number())\n#check missing data values\nfreedomData <- na.omit(freedomData) \ntrain <- freedomData %>% sample_frac(.8)\ntest <- anti_join(freedomData, train, by='id')\n\nytrain <- train$pf_ss_homicide\nytest <- test$pf_ss_homicide\nxtrainLin <- train %>% select(c(-\"id\"))\nxtrain <- train %>% select(c(-\"pf_ss_homicide\",-\"id\"))\nxtestLin <- test %>% select(c(-\"id\"))\nxtest <- test %>% select(c(-\"pf_ss_homicide\",-\"id\"))\n\nxtestFrame <- xtest\nxtest <- data.matrix(xtest)\npredictors <- data.matrix(xtrain)\nresp <- ytrain\n\nlinModel <- lm(pf_ss_homicide~., data=xtrainLin)\n\n \nThe figures above the show the outcome of trying to fit a traditional linear model to the data as it is. The model is not workable due to a lack of degrees of freedom to fit the number of predictors in the data set. In this case, the number of predictors could be trimmed at random to get a working model, but the regularization techniques can be used to evaluate this data set without randomly choosing predictors.\nTo fit the regularization models, the glmnet function will be used. The CV.glmnet function below performs a cross-fold validation on the training data in order to obtain the optimal lambda value for the respective model. In this function, the value of alpha controls the form of regularization that will be applied. An alpha value of 1 corresponds to LASSO regression, a value of 0 corresponds to Ridge Regression, and a value in between corresponds to a form of Elastic Net Regularization. The magnitude of alpha controls the penalty term in the Elastic Net Regularization. As in most regularization problems, the value of the penalty term (in this case lambda) is of critical importance.\n\n\n4.0.2 Fitting Regularized Models\nEach method will have a model fit. These will then have their RSquare and RMSE scores displayed to show a relative performance for each method.\nThe basic code structure is as follows:\n\nPerform Cross Validation to acquire the optimal lambda. The alpha term is altered based on the method.\nPass the optimal lambda into a new model based on the training data.\nExamine the impact on the coefficients.\nUsing the model, make predictions on the training data and the test data.\nStore results and display scores for each method.\n\n\nset.seed(250)\n#LASSO\nmodelResults <- data.frame(matrix(ncol=6,nrow=0))\ncolnames(modelResults)<-c(\"Model\",\"Train_RSquare\",\"Train_RMSE\",\"Test_RSquare\",\"Test_RMSE\",\"CoefficientCount\")\n\nmodel <- cv.glmnet(predictors, resp, alpha=1)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\nplot(model)\n\n\n\n\nThe LASSO Lambda plot demonstrates the MSE for different values of lambda.\n\nfinalModel <- glmnet(predictors,resp, alpha=1, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"LASSO\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\nLASSOCoef <- coefList\n\n\n#Ridge\nmodel <- cv.glmnet(predictors, resp, alpha=0)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\nplot(model)\n\n\n\n\nThe Ridge Regression plot shows higher values of lambda. For Ridge Regression, the choice of lambda represents a factor of shrinkage. A lambda of 0 would be equivalent to normal linear regression while a very high lambda would shrink all coefficients towards 0.\n\nfinalModel <- glmnet(predictors,resp, alpha=0, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"Ridge\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\n\nThree different models using elastic net will now be fit with different increments of alpha.\n\n#Elastic Net 1\nmodel <- cv.glmnet(predictors, resp, alpha=.25)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\n\n\nfinalModel <- glmnet(predictors,resp, alpha=.25, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"ENet.25\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\n\n\n#Elastic Net 2\nmodel <- cv.glmnet(predictors, resp, alpha=.5)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\n\n\nfinalModel <- glmnet(predictors,resp, alpha=.5, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"ENet.50\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\n\n#Elastic Net 3\nmodel <- cv.glmnet(predictors, resp, alpha=.75)\nbestLambda <- model$lambda.min\n#Optimal Lambda has been fit.\n\n\nfinalModel <- glmnet(predictors,resp, alpha=.75, lambda=bestLambda)\ncoefTable <- coefficients(finalModel)\ncoefList <- data.frame(matrix(ncol=2,nrow=0))\ncolnames(coefList)<-c(\"Predictor\",\"Coefficient\")\n\nfor(x in 1:nrow(coefTable)){\n if(coefTable[x,1] != 0)\n {rows <- nrow(coefList)\n predNames <- data.frame(coefTable@Dimnames)\n   newRow <- c(predNames[x,1],coefTable[x,1])\n    coefList[rows+1,] <- newRow    \n   } \n}\n\nfinalModelPredict <- predict(finalModel, s= bestLambda, newx = predictors)\nfinalModelTest <- predict(finalModel, s= bestLambda, newx = xtest)\nrows<-nrow(modelResults)\nnewRow <- c(\"ENet.75\",eval_results(resp,finalModelPredict,freedomData)$Rsquare,eval_results(resp,finalModelPredict,freedomData)$RMSE,eval_results(ytest,finalModelTest,freedomData)$Rsquare,eval_results(ytest,finalModelTest,freedomData)$RMSE, count(coefList))\nmodelResults[rows+1,]<-newRow\n\n\n\n4.0.3 Regularization Results / Discussion\nThe final model group looks like the table below.\n\nprint(modelResults)\n\n    Model Train_RSquare Train_RMSE Test_RSquare  Test_RMSE CoefficientCount\n1   LASSO     0.9990121 0.07226831    0.9981389 0.05918213               11\n2   Ridge     0.3022909 1.92055264    0.1656932 1.25305539               98\n3 ENet.25     0.9891321 0.23969644    0.9451068 0.32141539               40\n4 ENet.50     0.9965432 0.13518454    0.9912712 0.12816950               28\n5 ENet.75     0.9986039 0.08590949    0.9970629 0.07434730               16\n\n\nThe number of coefficients selected by each individual method corresponds to how “LASSO” like or “Ridge” like the alpha parameter was set. The pure LASSO regression shows that 13 predictors were selected, while the pure Ridge Regression shows all 98 possible predictors were chosen. The Elastic Net models fall somewhere between, depending on the strength of the alpha parameter.\nThe LASSO selected features included:\n\nprint(LASSOCoef)\n\n                         Predictor           Coefficient\n1                      (Intercept)     0.129676517850378\n2  pf_ss_disappearances_fatalities   -0.0146714340320808\n3             pf_ss_disappearances    -0.902231512843645\n4                      pf_ss_women    -0.953882920519535\n5                            pf_ss      2.85321435140525\n6           pf_religion_harassment   -0.0223026429291771\n7             pf_association_sport  -0.00317238900013525\n8             pf_expression_killed   0.00246240573210166\n9                   ef_legal_crime    0.0328255741798929\n10         ef_trade_movement_visit  -0.00196412760608385\n11               ef_trade_movement -0.000482527780599825\n\n\nThe model with the best fit in this case was the pure LASSO model. As has been shown, this regression model can be used for predictions. More refinement can be performed. For example, in this case, there is still the likelihood that the model is overfit since 13 predictors is still a large number when compared to 93 observations. To further understand this is a matter of the underlying mechanics in the data. It is also possible that these 13 predictors still have some manner of collinearity present. Regularization, in this case, has now given a workable group of features to analyze for a refined linear model.\n\n\n\n\n“An Introduction to ‘Glmnet‘.” 2021. stanford.edu. https://glmnet.stanford.edu/articles/glmnet.html."
  }
]