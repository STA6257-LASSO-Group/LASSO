# Methods and Materials

### Planned Demonstration

Using two different data sets, three different demonstrations will show the general method and special use cases for applying regularized regression techniques. First will be a general demonstration of the methods focused on model implementation and interpretation of the model's outputs. Second will be a showcase of applying regularized regression in a high multicollinearity example, and finally will be a demonstration of regularized regression in a high dimensional data setting where the number of features is greater than the number of observations.

### Data sets

Previous studies indicate LASSO regression outperforms Ridge regression with sparse data sets in which the variable size is larger than the sample size. Conversely, Ridge regression has been shown to outperform LASSO regression with dense data in which the variable size is smaller than the sample size.[@JamesTib21] The hybrid of the two, Elastic Net regression, will adapt to either type of data by using the properties of LASSO and Ridge regression together. We aim to use two data sets with different characteristics to highlight the advantages of each of the three linear regularization techniques while also using these statistical methods to model the data and make predictions.

### Human Freedom Index

The Human Freedom Index from the open intro R package will be used to demonstrate regularization in a high dimensional example where the number of predictors is greater than the number of observations. This data consists of sociological measures of the different types of freedom. The main dataset has 1458 observations with 123 variables, but for this example, a single year of complete observations will be used. This results in a dataset of 93 observations across 99 variables and mimics the situation that would occur if a researcher attempted to analyze the most recent year of this data in isolation from the rest of the dataset.

Aside from year, country, and region identifiers, each column in this data set is a continuous numerical variable. Some variables have high degrees of collinearity because of their relation to one another. For example, total disappearances is expected to have high collinearity with total violent disappearances.

### Jackson Heart Study

The Jackson Heart Study (JHS) is an on-going community-based observational study. The
data are collected from the medical histories, physical exams, and laboratory results of adult African Americans from three urban and rural counties in Jackson, Mississippi. There are 198 variables with an n of 2653 for the baseline exam Visit 1. After excluding redundant and categorical variables and imputing the data with MICE to account for missing values (see Programmatic Tools below) , we were left with 2653 observations and 27 variables (n\>p). The JHS data was used to highlight Lasso regression's performance over Ridge's when the sample size greatly outnumbers the variable size. Additionally, we used this data set to test all three methods on shrinking multicollinearity and to test LASSO's feature selection capabilities. Multicollinerity was measured by the Variance Inflation Factor (VIF). VIF is calculated by (1/1-R^2^) , where R^2^ is the sum of residuals squared divided by the total sum of squares of each individual x variable compared to all other x variables. A VIF score over 5 means that the variables are highly correlated. Tolerance is another way to measure multicollinearity and is the reciprocal of vif; high multicollinearity variables have a tolerance score above 0.2.

### Programmatic Tools, Statistical Methods, and Code Packages

RStudio is the primary IDE used to run the demonstrations of these methods. The R Language is the coding language used. To deal with missing values in the multicollinearity example, the missing values are imputed using the MICE (Multiple Imputation by Chained Equations) algorithm. This algorithm is able to determine appropriate values for missing data by using the other variables in the dataset to converge upon a satisfactory value. Since collinearity is an important aspect of the regularization problem, correlation plots will be used to visually showcase collinearity among variables where appropriate. The ones being used take the form of heat-maps where bright red color indicates high positive correlation and dark blue color indicates high negative correlation. [@mice] In addition, at the end of each demonstration, the R-squared and Root Mean Square Error will be calculated for the models run. The R-squared measures the among of predicted variable variance that is explained by the model while the error represents the standard deviation of the residuals. The Variance Inflation Factor (VIF) will be calculated in the multicollinearity example as it is a quantifier for how strong the multicollinearity effect is between variables. We will also be examining the difference between variable significance and importance where variable importance represents the amount of information a given predictor adds to the model and significance corresponds to whether or not a given predictor is explaining the random variation in a dependent variable. The primary code packages used to perform the Regularized Regression analyses will be two R packages: glmnet and caret. The glmnet package was developed at Stanford University by Trevor Hastie, Junyang Qian, and Kenneth Tay. It is designed to fit penalized generalized models and is able to calculate for the parameter lambda in an algorithmic approach (as opposed to having to provide a matrix of potential lambda variables). It is able to fit multiple regression situations, and, as such, has cross functional application beyond just the traditional linear LASSO regression implementation. In its arguments, this package can also be used to fit some of the extensions to LASSO such as relaxed LASSO. [@glmnet] Caret, by contrast, is a general classification and regression model package used in R. It is used for building models for a variety of problems, including generalized regression situations. In addition to regression problems, caret also contains methods for easily splitting test data, training models, and making predictions. [@caret] Additionally, we used the R package gamlr version 1.13-7 to perform the LASSO regression using a gamma distribution. The gamlr package is less widely used and documented than the glmnet package [@gamlr]. At this time, we were only able to run LASSO in gamlr as it's features are not as customizable as the glmnet package. So, these results were taken for more of a qualitative analysis for the JHS dataset, and no comparisons could be made with Ridge and Elastic Net.
